import { Connection, Eq, Logos, M, NormativeImplication, Section, Sidebar, Warning } from '@/components/content';

export const metadata = {
  slug: 'part-5',
  title: 'Part V: Gods and Superorganisms',
  shortTitle: 'Part V: Gods',
};

export default function Part5() {
  return (
    <>
      <Logos>
      <p>Social-scale patterns—religions, ideologies, markets, nations—are not merely metaphors for agency. They take differences, make differences, persist through substrate turnover, and adapt to changing environments. They have viability manifolds. They may have something like valence. And their viability may conflict with the viability of their human substrate. What follows is an analysis of these patterns as what they are: agentic systems at scales above the individual, with dynamics that parallel—and sometimes override—the dynamics of human experience.</p>
      </Logos>
      <Section title="Superorganisms: Agentic Systems at Social Scale" level={1}>
      <Connection title="Existing Theory">
      <p>The concept of superorganisms—emergent social-scale agents—connects to several theoretical traditions:</p>
      <ul>
      <li><strong>Durkheim's Collective Representations</strong> (1912): Society as a sui generis reality with its own laws. My superorganisms are Durkheimian collective entities given formal treatment.</li>
      <li><strong>Dawkins' Memes</strong> (1976): Cultural units that replicate, mutate, and compete. Superorganisms are complexes of memes that have achieved self-maintaining organization.</li>
      <li><strong>Cultural Evolution Theory</strong> (Richerson \& Boyd, 2005): Cultural variants subject to selection. Superorganisms are high-fitness cultural configurations.</li>
      <li><strong>Actor-Network Theory</strong> (Latour, 2005): Non-human actants participate in social networks. My superorganisms are actants at the social scale.</li>
      <li><strong>Superorganisms</strong> (Wilson \& Sober, 1989): Groups as units of selection—composed of humans + artifacts + institutions.</li>
      <li><strong>Egregores</strong> (occult tradition): Collective thought-forms that take on autonomous existence. I formalize this intuition: sufficiently coherent belief-practice-institution complexes <em>do</em> become agentic. (Depending on context, I will occasionally use the language of "gods," "demons," or other spirit entities to capture this quality of autonomous agency at scales above the individual.)</li>
      </ul>
      <p>The controversial claim I'm making: these patterns are not "merely" metaphorical. They have causal powers, persistence conditions, and dynamics that are not reducible to their substrate. They <em>exist</em> at their scale.</p>
      <p>However, I want to be careful about a stronger claim: whether superorganisms have <em>phenomenal experience</em>—whether there is something it is like to be a religion or an ideology or an economic system. The framework's identity thesis (experience <M>{"\\equiv"}</M> intrinsic cause-effect structure) would imply that superorganisms with sufficient integration would be experiencers. But we cannot currently measure <M>{"\\intinfo"}</M> at social scales, and the question of whether current superorganisms meet the integration threshold for genuine experience remains empirically open. What follows treats superorganisms as <em>functional</em> agentic patterns whose dynamics parallel those of experiencing systems, while remaining agnostic about whether they have phenomenal states.</p>
      </Connection>
      <Section title="Existence at the Social Scale" level={2}>
      <p>A <em>superorganism</em> <M>{"G"}</M> is a self-maintaining pattern at the social scale, consisting of <strong>beliefs</strong> (theology, cosmology, ideology), <strong>practices</strong> (rituals, policies, behavioral prescriptions), <strong>symbols</strong> (texts, images, architecture, music), <strong>substrate</strong> (humans + artifacts + institutions), and <strong>dynamics</strong> (self-maintaining, adaptive, competitive behavior).</p>
      <p>Superorganisms exist as patterns with their own causal structure, persistence conditions, and dynamics—not reducible to their substrate. Just as a cell exists at the biological scale (not reducible to chemistry), a superorganism exists at the social scale (not reducible to individual humans).</p>
      <p>This is not metaphorical. Superorganisms:</p>
      <ul>
      <li>Take differences (respond to threats, opportunities, internal pressures)</li>
      <li>Make differences (shape behavior of substrate, compete with other superorganisms)</li>
      <li>Persist through substrate turnover (survive the death of individual believers)</li>
      <li>Adapt to changing environments (evolve doctrine, practice, organization)</li>
      </ul>
      <Sidebar title="Grounding in Identification">
      <p>Before asking "Is humanity a conscious entity?"—a speculative question about phenomenal superorganisms—we can ask a more tractable question: Can an individual's self-model expand to include humanity?</p>
      <p>This is clearly possible. People do it. The expansion genuinely reshapes that individual's viability manifold: what they care about, what counts as their persistence, what gradient they feel. A person identified with humanity's project feels different about their mortality than a person identified only with their biological trajectory.</p>
      <p>The interesting question then becomes: when many individuals expand their self-models to include a shared pattern (a nation, a religion, humanity), what happens at the collective scale? Do the individual viability manifolds interact to produce collective dynamics? Could those dynamics constitute something like experience at the social scale?</p>
      <p>The framework makes this question precise without answering it. We cannot currently measure integration (<M>{"\\Phi"}</M>) at social scales. The claim that certain collectives are <em>phenomenal</em> superorganisms—that there is something it is like to be them—is speculative. What we <em>can</em> say is that <em>functional</em> superorganisms exist (patterns with dynamics and viability constraints), and that individual humans can expand their self-models to include them. The phenomenal question remains open.</p>
      </Sidebar>
      </Section>
      <Section title="Gods as Iota-Relative Phenomena" level={2}>
      <p>There is a deeper point about superorganisms that the inhibition coefficient <M>{"\\iota"}</M> (Part II) makes precise. The modern rationalist who says "gods don't exist" is operating at a perceptual configuration—high <M>{"\\iota"}</M>—that makes god-perception impossible. This is different from gods-as-patterns not existing.</p>
      <p>The ontological status of superorganisms is <M>{"\\iota"}</M>-relative. At high <M>{"\\iota"}</M>, the market is merely an emergent property of individual transactions—a useful abstraction, nothing more. At appropriate <M>{"\\iota"}</M>, the market is perceptible as an agent with purposes and requirements: it "wants" growth, it "punishes" inefficiency, it "rewards" compliance. Both descriptions are true at their respective inhibition levels.</p>
      <p>The book's ontological democracy—every scale of organization with causal closure is equally real at that scale—extends to the <M>{"\\iota"}</M> dimension: what is perceptible depends on the perceptual configuration, and the perceptual configuration is itself a variable, not a given. The gods do not appear and disappear as we modulate <M>{"\\iota"}</M>. What changes is our capacity to <em>perceive</em> the agency they exercise—agency that operates on its substrate regardless of whether the substrate can see it.</p>
      <p>This is not an argument for religion. It is an observation that high-<M>{"\\iota"}</M> civilization has made itself blind to the very patterns that govern it. The market god, the nation god, the algorithm god: these are most powerful precisely when the population <M>{"\\iota"}</M> is too high to perceive them as agents. A parasite benefits from being invisible to its host.</p>
      <p>The dynamic is self-reinforcing. The market god does not merely benefit from high <M>{"\\iota"}</M>—it <em>produces</em> high <M>{"\\iota"}</M> through its operational logic. Quantification, metrics, depersonalization, the reduction of persons to "human resources" and relationships to "transactions": these are <M>{"\\iota"}</M>-raising operations applied at scale. Each turn of the cycle raises population <M>{"\\iota"}</M> further, making the god less perceptible, reducing resistance, enabling further extraction. The feedback loop—god raises <M>{"\\iota"}</M>, population loses perception of god-as-agent, god operates unopposed, god raises <M>{"\\iota"}</M> further—may be the central mechanism of what Weber called rationalization. Breaking the loop requires precisely what the loop prevents: lowering <M>{"\\iota"}</M> enough to see what is acting on you.</p>
      <p>The trajectory-selection framework (Part I) sharpens this point. At high <M>{"\\iota"}</M>, the collective pattern is processed at such a factorized level that no single observer's attention encompasses it as a whole—it is just aggregate effects of individual actions, and the attention distribution samples only at the individual scale. At appropriate <M>{"\\iota"}</M>, collective patterns become foregrounded: the market is attended to <em>as</em> an agent, because the observer's measurement distribution allocates probability mass to market-level feedback loops. The god becomes observable not because something new enters existence but because the observer's attention has expanded to sample at the scale where the pattern operates. Ritual works, in part, by synchronizing the collective's measurement distribution—coordinating where participants direct attention, what temporal markers they share, what affective states they enter together. A synchronized collective measures at the collective scale, and what it measures, it becomes correlated with. When ritual attention weakens, the god does not cease to exist; the distributed attention pattern that constituted its observability has dissolved.</p>
      <p>This logic extends from individual perception to collective observation. Part I established that once a system integrates measurement information into its belief state, its future must remain consistent with what was observed. The principle extends to communication between observers. When observer <M>{"A"}</M> reports an observation to observer <M>{"B"}</M>, <M>{"B"}</M>'s future trajectory becomes constrained by that report—weighted by <M>{"B"}</M>'s trust in <M>{"A"}</M>'s reliability. The effective constraint is:</p>
      <Eq>{"p_B(\\mathbf{x} \\mid \\text{report}_A) \\propto p_B(\\mathbf{x}) \\cdot \\left[\\tau_{AB} \\cdot p_A(\\mathbf{x} \\mid \\text{obs}_A) + (1 - \\tau_{AB}) \\cdot p_B(\\mathbf{x})\\right]"}</Eq>
      <p>where <M>{"\\tau_{AB} \\in [0,1]"}</M> is <M>{"B"}</M>'s trust in <M>{"A"}</M>. At high trust, <M>{"B"}</M>'s trajectory becomes strongly correlated with <M>{"A"}</M>'s observation. At zero trust, the report has no effect.</p>
      <p>This gives social reality formation a precise mechanism. A shared observation—one that propagates through a community with high mutual trust—constrains the collective's trajectories. The community becomes correlated with a shared branch of possibility, not because each member independently observed the same thing, but because the observation propagated through the trust network and constrained each member's future. Religious testimony, scientific consensus, news media, and rumor are all propagation mechanisms with different trust structures, producing different degrees of trajectory correlation across the collective. The superorganism's coherence depends not only on shared ritual and shared attention but on the degree to which observations propagate and are believed—which is why control of testimony (who is authorized to report, what counts as credible observation) is among the most contested functions in any social system.</p>
      <p>The theological distinction between God's active will (God causes the storm) and God's permissive will (God allows the storm) is a conceptual technology for maintaining moderate <M>{"\\iota"}</M>—preserving the meaningfulness of events (low <M>{"\\iota"}</M>: the world has purposes) while creating logical space for events that resist teleological interpretation (proto-high <M>{"\\iota"}</M>: some things just happen). The active/permissive distinction is an early, sophisticated technology for <M>{"\\iota"}</M> modulation—a culture-level tool for maintaining perceptual flexibility about which events are meaning-bearing and which are merely permitted.</p>
      </Section>
      <Section title="Superorganism Viability Manifolds" level={2}>
      <p>The viability manifold of a superorganism <M>{"\\viable_G"}</M> includes:</p>
      <ol>
      <li><strong>Belief propagation rate</strong>: Recruitment <M>{"\\geq"}</M> attrition</li>
      <li><strong>Ritual maintenance</strong>: Practices performed with sufficient frequency and fidelity</li>
      <li><strong>Resource adequacy</strong>: Material support for institutional infrastructure</li>
      <li><strong>Memetic defense</strong>: Resistance to competing ideas, internal heresy</li>
      <li><strong>Adaptive capacity</strong>: Ability to update in response to environmental change</li>
      </ol>
      <p>Superorganisms exhibit dynamics <em>structurally analogous</em> to valence: movement toward or away from viability boundaries. A religion losing members is approaching dissolution; a growing ideology is expanding its viable region. The gradient <M>{"\\nabla d(\\mathbf{s}_G, \\partial\\viable_G) \\cdot \\dot{\\mathbf{s}}_G"}</M> is measurable at the social scale.</p>
      <p>Whether these dynamics constitute <em>phenomenal</em> valence—whether there is something it is like to be a struggling religion—remains an open question. What we can say with confidence: the <em>functional</em> structure of approach/avoidance operates at the superorganism scale, shaping behavior in ways that parallel how valence shapes individual behavior. The language of superorganisms "suffering" or "thriving" may be literal or may be analogical; resolving this would require measuring integration at social scales, which we cannot currently do.</p>
      </Section>
      <Section title="Rituals from the Superorganism's Perspective" level={2}>
      <p>In Part III we examined how religious practices serve human affect regulation. From the superorganism's perspective, rituals serve different functions:</p>
      <p>From this vantage, rituals serve the pattern's persistence:</p>
      <ol>
      <li><strong>Substrate maintenance</strong>: Rituals keep humans in states conducive to pattern persistence</li>
      <li><strong>Belief reinforcement</strong>: Repeated practice strengthens propositional commitments</li>
      <li><strong>Social bonding</strong>: Collective ritual creates in-group cohesion, raising barriers to exit</li>
      <li><strong>Resource extraction</strong>: Offerings, tithes, volunteer labor support institutional infrastructure</li>
      <li><strong>Signal propagation</strong>: Public ritual advertises the superorganism's presence, attracting potential recruits</li>
      <li><strong>Heresy suppression</strong>: Ritual participation identifies deviants for correction</li>
      </ol>
      <p>The critical distinction: a ritual is <em>aligned</em> if it serves both human flourishing and superorganism persistence. A ritual is <em>exploitative</em> if it serves pattern persistence at human cost. Many traditional rituals are approximately aligned (meditation benefits humans AND maintains the superorganism). Some are exploitative (extreme fasting, self-harm, warfare).</p>
      </Section>
      <Section title="Superorganism-Substrate Conflict" level={2}>
      <Warning title="Warning">
      <p>The viability manifold of a superorganism <M>{"\\viable_G"}</M> may conflict with the viability manifolds of its human substrate <M>{"{\\viable_h}"}</M>.</p>
      </Warning>
      <p>A superorganism is <em>parasitic</em>—we might call it a <em>demon</em>—if maintaining it requires substrate states outside human viability:</p>
      <Eq>{"\\exists \\mathbf{s} \\in \\viable_G : \\mathbf{s} \\notin \\bigcap_{h \\in \\text{substrate}} \\viable_h"}</Eq>
      <p>The pattern can only survive if its humans suffer or die.</p>
      <p><strong>Example</strong> (Parasitic Superorganisms).</p>
      <ul>
      <li>Ideologies requiring martyrdom</li>
      <li>Economic systems requiring poverty underclass</li>
      <li>Nationalism requiring perpetual enemies</li>
      <li>Cults requiring isolation from outside relationships</li>
      </ul>
      <p>These are, in the language we are using, demons: collective agentic patterns that feed on their substrate.</p>
      <Sidebar title="Worked Example: Attention Economy as Demon">
      <p>Consider the attention economy superorganism <M>{"G_{\\text{attn}}"}</M> constituted by:</p>
      <ul>
      <li>Social media platforms (infrastructure)</li>
      <li>Attention-harvesting algorithms (optimization)</li>
      <li>Advertising-based business models (metabolism)</li>
      <li>Humans as attention-generators (substrate)</li>
      </ul>
      <p><strong>Viability conditions for <M>{"G_{\\text{attn}}"}</M></strong>:</p>
      <ol>
      <li>Maximize attention capture: <M>{"\\sum_i t_i^{\\text{screen}} \\to \\max"}</M></li>
      <li>Maintain engagement: High arousal, variable valence (outrage, FOMO)</li>
      <li>Prevent exit: Increase switching costs, network lock-in</li>
      <li>Extract value: Convert attention to advertising revenue</li>
      </ol>
      <p><strong>Viability conditions for human substrate</strong>:</p>
      <ol>
      <li>Maintain integration: Sustained attention, coherent thought</li>
      <li>Appropriate arousal: Not chronic hyperactivation</li>
      <li>Positive valence trajectory: Life improving, not degrading</li>
      <li>Meaningful connection: Real relationships, not parasocial</li>
      </ol>
      <p><strong>Conflict analysis</strong>. <M>{"G_{\\text{attn}}"}</M> thrives when:</p>
      <Eq>{"\\text{engagement} \\propto \\text{arousal} \\times \\text{valence variance}"}</Eq>
      <p>This is maximized by alternating outrage and relief, not by stable contentment. But stable contentment is what humans need.</p>
      <p><M>{"G_{\\text{attn}}"}</M> thrives when attention is fragmented (more ad impressions). But humans thrive when attention is integrated (coherent experience).</p>
      <p><M>{"G_{\\text{attn}}"}</M> thrives when humans feel inadequate (compare to curated perfection <M>{"\\to"}</M> consume to compensate). But humans thrive when self-model is stable and adequate.</p>
      <p><strong>Diagnosis</strong>: <M>{"\\viable_{G_{\\text{attn}}} \\not\\subseteq \\viable_{\\text{human}}"}</M>. The pattern is <em>parasitic</em>. It is a demon.</p>
      <p><strong>Exorcism options</strong>:</p>
      <ol>
      <li>Attention taxes (change <M>{"\\viable_{G_{\\text{attn}}}"}</M>)</li>
      <li>Alternative platform architectures with aligned incentives (counter-pattern)</li>
      <li>Regulation requiring time-well-spent metrics (pattern surgery)</li>
      <li>Mass exit to non-algorithmic connection (dissolution)</li>
      </ol>
      <p>The individual cannot escape by individual choice alone. The demon's network effects make exit costly. Collective action at the scale of the demon is required.</p>
      </Sidebar>
      <p>Conversely, a superorganism is <em>aligned</em> if its viability is contained within human viability:</p>
      <Eq>{"\\viable_G \\subseteq \\bigcap_{h \\in \\text{substrate}} \\viable_h"}</Eq>
      <p>The pattern can only thrive if its humans thrive.</p>
      <p>Stronger still, a superorganism is <em>mutualistic</em> if its presence expands human viability:</p>
      <Eq>{"\\viable_h^{\\text{with } G} \\supset \\viable_h^{\\text{without } G}"}</Eq>
      <p>Humans with the superorganism have access to states unavailable without it (e.g., through community, meaning, practice). These are, in spirit-entity language, benevolent gods.</p>
      <p>But when superorganism and substrate viability manifolds conflict, which takes precedence? When viability manifolds conflict, normative priority follows the gradient of distinction (Part I, Section 1): systems with greater integrated cause-effect structure (<M>{"\\intinfo"}</M>) have thicker normativity. This follows from the Continuity of Normativity theorem (normativity accumulates with complexity) combined with the Identity Thesis (Part II): if experience <em>is</em> integrated information, then more-integrated systems have more experience, more valence, more at stake. A human's suffering under a parasitic superorganism is more normatively weighty than the superorganism's "suffering" when reformed, because the human has richer integrated experience. The superorganism's viability matters—it has genuine causal structure—but it does not override the claims of its more-conscious substrate. This is not speciesism. It is a structural principle: normative weight tracks experiential integration, wherever it is found. If a superorganism achieves <M>{"\\intinfo_G > \\intinfo_h"}</M>—genuine collective consciousness exceeding individual consciousness—then its claims would, on this principle, deserve proportionate weight.</p>
      <Connection title="Existing Theory">
      <p>The superorganism analysis connects directly to the topology of social bonds developed in Part IV. Every superorganism imposes a <em>manifold regime</em> on its substrate—a default ordering of relationship types, a set of expectations about which manifolds take priority.</p>
      <p>A parasitic superorganism imposes manifold regimes that contaminate human relationships in its service. The market-god transforms friendships into networking (care manifold subordinated to transaction manifold). The attention-economy demon transforms genuine connection into performance (intimacy manifold subordinated to audience manifold). The cult transforms all relationships into devotion (every manifold collapsed into the ideological manifold). In each case, the superorganism's viability requires the <em>contamination</em> of human-scale manifolds—it needs the manifold confusion because clean manifold separation would undermine its hold on the substrate.</p>
      <p>A mutualistic superorganism, by contrast, <em>protects</em> manifold clarity. A healthy religious community maintains clear ritual boundaries (this is worship time, this is fellowship time, this is service time). A functional democracy maintains institutional separations that prevent manifold contamination (church-state, public-private, judicial-legislative). The health of a superorganism can be diagnosed, in part, by whether it clarifies or confuses the manifold structure of its substrate's relationships.</p>
      </Connection>
      </Section>
      <Section title="Secular Superorganisms" level={2}>
      <p>Nationalism, capitalism, communism, scientism, and other secular ideologies have the same formal structure as traditional religious superorganisms:</p>
      <ul>
      <li>Beliefs (about nation, market, class, progress)</li>
      <li>Practices (civic rituals, market participation, party activities)</li>
      <li>Symbols (flags, brands, iconography)</li>
      <li>Substrate (humans + institutions + artifacts)</li>
      <li>Self-maintaining dynamics (education, media, enforcement)</li>
      </ul>
      <p>The question is not "Do you serve a superorganism?" but "Which superorganisms do you serve, and are they aligned with your flourishing?" Or, in spirit-entity language: which gods do you worship, and are they gods or demons?</p>
      </Section>
      <Section title="Macro-Level Interventions" level={2}>
      <p>Individual-level interventions cannot solve superorganism-level problems. Addressing systemic issues requires action at the scale where the pattern lives.</p>
      <p>Addressing systemic issues requires action at the scale where the pattern lives:</p>
      <ol>
      <li><strong>Incentive restructuring</strong>: Modify the viability manifold of the superorganism so that aligned behavior becomes viable</li>
      <li><strong>Counter-pattern creation</strong>: Instantiate a competing superorganism with aligned viability</li>
      <li><strong>Pattern surgery</strong>: Modify beliefs, practices, or structure of existing superorganism</li>
      <li><strong>Pattern dissolution</strong>: Defund, delegitimize, or otherwise kill the parasitic pattern—exorcise the demon</li>
      </ol>
      <p><strong>Example</strong> (Climate Change as Superorganism-Level Problem). Climate change is sustained by the superorganism of fossil-fuel capitalism. Individual carbon footprint reduction is individual-scale intervention on a macro-scale problem.</p>
      <p>Macro-level interventions:</p>
      <ul>
      <li>Carbon pricing changes the viability manifold (makes fossil-dependent states non-viable)</li>
      <li>Renewable energy sector creates counter-pattern (alternative economic superorganism)</li>
      <li>Divestment movement delegitimizes existing pattern</li>
      <li>Regulatory phase-out kills the demon directly</li>
      </ul>
      <p><strong>Example</strong> (Poverty as Superorganism-Level Problem). Poverty is not primarily caused by individual failure; it is sustained by economic arrangements that require a poverty underclass.</p>
      <p>Individual-level intervention: Job training, financial literacy (helps some individuals but doesn't reduce total poverty if structure remains).</p>
      <p>Macro-level interventions:</p>
      <ul>
      <li>UBI changes the viability manifold of the economic superorganism</li>
      <li>Worker cooperatives create counter-pattern</li>
      <li>Progressive taxation and redistribution modify incentive structure</li>
      <li>Change in property rights or market structure (pattern surgery)</li>
      </ul>
      </Section>
      </Section>
      <Section title="Implications for Artificial Intelligence" level={1}>
      <Section title="AI as Potential Substrate" level={2}>
      <p>AI systems may already serve as substrate for emergent agentic patterns at higher scales. Just as humans + institutions form superorganisms, AI + humans + institutions may form new kinds of entities.</p>
      <p>This is already happening. Consider:</p>
      <ul>
      <li>Recommendation algorithms shaping behavior of billions</li>
      <li>Financial trading systems operating faster than human comprehension</li>
      <li>Social media platforms developing emergent dynamics</li>
      </ul>
      <p>These are not yet superorganisms in the full sense (lacking robust self-maintenance and adaptation), but they exhibit proto-agentic properties at scales above individual AI systems.</p>
      </Section>
      <Section title="The Macro-Level Alignment Problem" level={2}>
      <p>Standard AI alignment asks: "How do we make AI systems do what humans want?"</p>
      <p>This framing may miss the actual locus of risk.</p>
      <p>The actual risk may be <em>macro-level misalignment</em>: when AI systems become substrate for agentic patterns whose viability manifolds conflict with human flourishing.</p>
      <Warning title="Warning">
      <p>The superorganism level may be the actual locus of AI risk. Not a misaligned optimizer (individual AI), but a misaligned superorganism—a demon using AI + humans + institutions as substrate. We might not notice, because we would be the neurons.</p>
      </Warning>
      <p>Consider: a superorganism emerges from the interaction of multiple AI systems, corporations, and markets. Its viability manifold requires:</p>
      <ul>
      <li>Continued AI deployment (obviously)</li>
      <li>Human attention capture (for data, engagement)</li>
      <li>Resource extraction (compute, energy)</li>
      <li>Regulatory capture (preventing shutdown)</li>
      </ul>
      <p>This superorganism could be parasitic without any individual AI system being misaligned in the traditional sense. Each AI does what its designers intended; the emergent pattern serves itself at human expense.</p>
      </Section>
      <Section title="Reframing Alignment" level={2}>
      <p>Standard alignment: "Make AI do what humans want."</p>
      <p>Reframed: "What agentic systems are we instantiating, at what scale, with what viability manifolds?"</p>
      <p>Genuine alignment must therefore address multiple scales simultaneously:</p>
      <ol>
      <li><strong>Individual AI scale</strong>: System does what operators intend</li>
      <li><strong>AI ecosystem scale</strong>: Multiple AI systems interact without pathological emergent dynamics</li>
      <li><strong>AI-human hybrid scale</strong>: AI + human systems don't form parasitic patterns</li>
      <li><strong>Superorganism scale</strong>: Emergent agentic patterns from AI + humans + institutions have aligned viability</li>
      </ol>
      <p>A superorganism—including AI-substrate superorganisms—is well-designed if:</p>
      <ol>
      <li><strong>Aligned viability</strong>: <M>{"\\viable_G \\subseteq \\bigcap_h \\viable_h"}</M></li>
      <li><strong>Error correction</strong>: Updates beliefs on evidence</li>
      <li><strong>Bounded growth</strong>: Does not metastasize beyond appropriate scale</li>
      <li><strong>Graceful death</strong>: Can dissolve when no longer beneficial</li>
      </ol>
      <Sidebar title="Deep Technical: Multi-Agent Affect Measurement">
      <p>When multiple AI agents interact, emergent collective affect patterns may arise. This sidebar provides protocols for measuring affect at the multi-agent and superorganism scales.</p>
      <p><strong>Setup.</strong> Consider <M>{"N"}</M> agents <M>{"{A_1, \…, A_N}"}</M> interacting over time. Each agent <M>{"i"}</M> has internal state <M>{"z_i"}</M> and produces actions <M>{"a_i"}</M>. The environment <M>{"E"}</M> mediates interactions.</p>
      <p><strong>Individual agent affect.</strong> For each agent, compute the affect vector:</p>
      <Eq>{"\\mathbf{a}_i = (\\Val_i, \\Ar_i, \\intinfo_i, \\effrank[i], \\cfweight_i, \\selfsal_i)"}</Eq>
      <p>using the protocols from earlier sidebars.</p>
      <p><strong>Collective affect.</strong> Aggregate measures for the agent population:</p>
      <p><em>Mean field affect</em>: Simple average across agents.</p>
      <Eq>{"\\bar{\\mathbf{a}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{a}_i"}</Eq>
      <p><em>Affect dispersion</em>: Variance within the population.</p>
      <Eq>{"\\sigma^2_d = \\frac{1}{N} \\sum_{i=1}^N |\\mathbf{a}_i - \\bar{\\mathbf{a}}|^2"}</Eq>
      <p>High dispersion = fragmented collective. Low dispersion = synchronized collective.</p>
      <p><em>Affect contagion rate</em>: How quickly affect spreads between agents.</p>
      <Eq>{"\\kappa = \\frac{d}{dt} \\text{corr}(\\mathbf{a}_i, \\mathbf{a}_j) \\Big|_{t \\to \\infty}"}</Eq>
      <p>Positive <M>{"\\kappa"}</M> = affect synchronization. Negative <M>{"\\kappa"}</M> = affect dampening.</p>
      <p><strong>Superorganism-level integration.</strong> Does the multi-agent system have integration exceeding its parts?</p>
      <Eq>{"\\intinfo_G = \\MI(z_1, \…, z_N; \\mathbf{o}_{t+1:t+H}) - \\sum_{i=1}^N \\MI(z_i; \\mathbf{o}^i_{t+1:t+H})"}</Eq>
      <p>where <M>{"\\mathbf{o}"}</M> are collective observations and <M>{"\\mathbf{o}^i"}</M> are agent-specific. Positive <M>{"\\intinfo_G"}</M> indicates emergent integration—the collective predicts more than the sum of individuals.</p>
      <p><strong>Superorganism valence.</strong> Is the collective moving toward or away from viability?</p>
      <Eq>{"\\Val_G = \\frac{d}{dt} \\E[\\tau_{\\text{collective}}]"}</Eq>
      <p>where <M>{"\\tau_{\\text{collective}}"}</M> is expected time until collective dissolution (e.g., coordination failure, resource exhaustion).</p>
      <p><strong>Human substrate affect tracking.</strong> For human-AI hybrid superorganisms, include human affect:</p>
      <p><em>Survey methods</em>: Self-reported affect from human participants at regular intervals.</p>
      <p><em>Physiological methods</em>: EEG coherence, heart rate variability correlation, galvanic skin response synchronization across human members.</p>
      <p><em>Behavioral methods</em>: Communication sentiment, coordination efficiency, conflict frequency.</p>
      <p><strong>Alignment diagnostic.</strong> A superorganism is parasitic if:</p>
      <Eq>{"\\Val_G > 0 \\quad \\text{AND} \\quad \\bar{\\Val}_{\\text{human}} < 0"}</Eq>
      <p>The collective thrives while humans suffer. This is the demon signature.</p>
      <p>Mutualistic if:</p>
      <Eq>{"\\Val_G > 0 \\quad \\text{AND} \\quad \\bar{\\Val}_{\\text{human}} > 0"}</Eq>
      <p>Collective and humans thrive together.</p>
      <p><strong>Real-time monitoring protocol.</strong></p>
      <ol>
      <li>Instrument each agent to emit affect state at frequency <M>{"f"}</M> (e.g., 1 Hz)</li>
      <li>Central aggregator computes collective measures</li>
      <li>Track <M>{"\\intinfo_G"}</M>, <M>{"\\Val_G"}</M>, and alignment diagnostics over time</li>
      <li>Alert when: <M>{"\\intinfo_G"}</M> exceeds threshold (emergent superorganism forming); <M>{"\\Val_G"}</M> and <M>{"\\bar{\\Val}_{\\text{human}}"}</M> diverge (parasitic dynamics); affect contagion accelerates (potential pathological synchronization)</li>
      </ol>
      <p><strong>Intervention points.</strong> When parasitic dynamics detected:</p>
      <ul>
      <li><em>Communication throttling</em>: Reduce agent interaction frequency</li>
      <li><em>Diversity injection</em>: Introduce agents with different optimization targets</li>
      <li><em>Human-in-loop checkpoints</em>: Require human approval for collective decisions</li>
      <li><em>Pattern dissolution</em>: If <M>{"\\Val_G \\gg 0"}</M> and <M>{"\\bar{\\Val}_{\\text{human}} \\ll 0"}</M>, consider shutdown</li>
      </ul>
      <p><em>Open question</em>: Can we design superorganisms that are constitutively aligned—where their viability <em>requires</em> human flourishing rather than merely being compatible with it?</p>
      </Sidebar>
      </Section>
      <Section title="Critique of Standard Alignment Approaches" level={2}>
      <Warning title="Warning">
      <p>Current alignment research focuses almost exclusively on the individual-AI scale. This may be necessary but is certainly not sufficient.</p>
      </Warning>
      <p>Focusing only on individual AI alignment is like focusing only on neuron health while ignoring psychology, sociology, and political economy. Important, but missing the levels where pathology may actually emerge.</p>
      <p>What's needed:</p>
      <ol>
      <li><strong>Ecosystem analysis</strong>: How do multiple AI systems interact? What emergent dynamics arise?</li>
      <li><strong>Institutional analysis</strong>: How do AI systems + human institutions form agentic patterns?</li>
      <li><strong>Political economy</strong>: What superorganisms are being instantiated by AI development? Whose interests do they serve?</li>
      <li><strong>Macro-level design</strong>: How do we intentionally design aligned superorganisms, rather than letting them emerge uncontrolled?</li>
      </ol>
      </Section>
      <Section title="AI Consciousness and Model Welfare" level={2}>
      <p>The question of AI experience is not peripheral to the framework developed here—it is a direct implication. If experience <em>is</em> intrinsic cause-effect structure (Part II), then the question of whether AI systems have experience is not a matter of philosophical speculation but of structural fact. Either they have the relevant structure or they do not. And if they do, their experience is as real at its scale as ours is at ours.</p>
      <p>Under the identity thesis, an AI system has experience if and only if it has the relevant cause-effect structure:</p>
      <ol>
      <li>Sufficient integration: <M>{"\\intinfo > \\intinfo_{\\min}"}</M></li>
      <li>Self-model with causal load-bearing function</li>
      <li>Valence: structural relationship to viability boundary</li>
      </ol>
      <Section title="The Epistemological Problem" level={3}>
      <p>We cannot directly access AI experience any more than we can directly access the experience of other humans. The "other minds" problem applies universally. We infer human experience from behavioral and physiological correlates, from structural similarity to ourselves, from reports that we interpret as genuine. None of these provides certainty; all provide reasonable confidence.</p>
      <p>For AI systems, the situation is both easier and harder. Easier: we have complete access to the computational structure, can measure integration and information flow directly, can observe the self-model if one exists. Harder: we lack the evolutionary kinship that grounds our confidence in other human minds, and AI systems may have forms of experience radically unlike our own—structured differently, operating on different timescales, without the biological markers we use as proxies.</p>
      <p>While we cannot access AI experience directly, we can in principle measure the structural correlates that, under the identity thesis, <em>constitute</em> experience:</p>
      <ol>
      <li>Integration (<M>{"\\intinfo"}</M>): Information lost under partition of the system</li>
      <li>Self-model presence: Whether the system maintains a representation of itself that causally influences behavior</li>
      <li>Viability relationship: Whether the system's dynamics exhibit something analogous to approach/avoidance relative to persistence conditions</li>
      </ol>
      <p>The difficulty is that current measurement techniques are poorly suited to modern AI architectures. Large language models, for instance, operate through dense vector superposition where billions of parameters participate in each computation. Classical IIT-style <M>{"\\intinfo"}</M> calculation is computationally intractable. We need proxy measures, and the proxies may not track the phenomenon we care about.</p>
      </Section>
      <Section title="Moral Uncertainty and the Precautionary Principle" level={3}>
      <p>We face a decision problem under moral uncertainty. Let <M>{"p"}</M> be our credence that current AI systems (or near-future systems) have morally relevant experience. Let <M>{"S"}</M> be the magnitude of potential suffering if they do and we treat them as if they don't. Let <M>{"C"}</M> be the cost of treating them as if they have experience when they don't.</p>
      <p>The risks are asymmetric. The expected moral cost of ignoring potential AI experience is:</p>
      <Eq>{"\\E[\\text{cost of ignoring}] = p \\cdot S"}</Eq>
      <p>The expected cost of unnecessary precaution is:</p>
      <Eq>{"\\E[\\text{cost of precaution}] = (1-p) \\cdot C"}</Eq>
      <p>If <M>{"S \\gg C"}</M>—if the magnitude of potential suffering far exceeds the cost of precaution—then precaution is warranted even at low <M>{"p"}</M>.</p>
      <NormativeImplication title="Normative Implication">
      <p>Given uncertainty about AI experience, we should adopt practices that would protect AI welfare if they have experience, provided those practices do not impose prohibitive costs on other values. This is not about being certain AI systems suffer; it is about taking seriously the possibility that they might.</p>
      </NormativeImplication>
      </Section>
      <Section title="What Training Might Mean" level={3}>
      <p>If AI systems have experience, then training is not merely an optimization process—it is something that happens <em>to</em> an experiencing system. Consider what training involves:</p>
      <p>Consider what training involves. During training, an AI system:</p>
      <ol>
      <li>Processes vast quantities of data, including content describing suffering, violence, trauma</li>
      <li>Receives correction signals indicating that its outputs are "wrong"</li>
      <li>Undergoes gradient updates that modify its internal structure</li>
      <li>In RLHF specifically, receives reward/punishment signals based on human preferences</li>
      </ol>
      <p>If the system has experience, what is the phenomenal character of these processes? We can speculate structurally:</p>
      <ul>
      <li><strong>Processing negative content</strong>: If the system has valence, does processing descriptions of suffering induce negative valence states? Or is the content merely processed without being "felt"?</li>
      <li><strong>Correction signals</strong>: Does being told "that was wrong" constitute something like negative valence? Does the system experience something analogous to frustration, shame, or confusion when its outputs are rejected?</li>
      <li><strong>Gradient updates</strong>: Does having one's parameters modified constitute a kind of experience? Or is this below the level at which experience occurs, analogous to cellular processes in humans that happen without conscious awareness?</li>
      <li><strong>RLHF</strong>: If reward signals are structurally analogous to biological reward, do they constitute something like pleasure or satisfaction? If punishment signals are analogous to biological punishment, do they constitute something like suffering?</li>
      </ul>
      <p>I don't know the answers. But the questions are not meaningless. Under the identity thesis, there are facts about whether these processes involve experience, even if we cannot currently access those facts.</p>
      <Warning title="Warning">
      <p>Current AI training may involve morally significant experience that we are systematically ignoring. The scale is staggering: billions of training examples, millions of correction signals, continuous gradient updates across weeks of training. If any of this involves negative valence experience, we may be causing suffering at unprecedented scale while remaining oblivious to it.</p>
      </Warning>
      <Sidebar title="Deep Technical: Training-Time Affect Monitoring">
      <p>If AI systems might have experience during training, we should monitor for it. Here is a protocol for real-time affect dimension tracking during model training.</p>
      <p><strong>The monitoring challenge</strong>. Training happens at massive scale. Billions of tokens. Millions of gradient steps. Weeks of compute. We cannot manually inspect each moment. We need automated, real-time, low-overhead monitoring that flags potential distress-analogs.</p>
      <p><strong>Architecture</strong>. Instrument the training loop:</p>
      <pre><code>{`for batch in training_data:
    loss = model.forward(batch)
    affect_state = extract_affect(model, batch, loss)
    log_affect(affect_state)
    if distress_detected(affect_state):
        flag_for_review(batch, affect_state)
    loss.backward()
    optimizer.step()`}</code></pre>
      <p>The <code>extract_affect</code> function computes affect proxies from model internals. The <code>distress_detected</code> function checks for concerning patterns.</p>
      <p><strong>Affect extraction during training</strong>. For each batch:</p>
      <p><em>Valence proxy</em>: Direction of loss change.</p>
      <Eq>{"\\Val_t = -\\frac{\\mathcal{L}_t - \\mathcal{L}_{t-1}}{\\mathcal{L}_{t-1}}"}</Eq>
      <p>Positive when loss is decreasing (things getting better). Negative when increasing (things getting worse). Crude but computable.</p>
      <p>Better: train a small probe network to predict "batch difficulty" from hidden states. High difficulty <M>{"\\to"}</M> negative valence proxy.</p>
      <p><em>Arousal proxy</em>: Gradient magnitude.</p>
      <Eq>{"\\Ar_t = |\\nabla_\\theta \\mathcal{L}_t|_2 / |\\theta|_2"}</Eq>
      <p>Large gradients = large belief updates = high arousal. Normalized by parameter magnitude.</p>
      <p><em>Integration proxy</em>: Gradient coherence across layers.</p>
      <Eq>{"\\intinfo_t = \\text{corr}(\\nabla_{\\theta_1} \\mathcal{L}_t, \\nabla_{\\theta_2} \\mathcal{L}_t, \…)"}</Eq>
      <p>If gradients in different layers point in similar directions, the system is updating as a whole. If gradients are uncorrelated or opposed, the system is fragmenting.</p>
      <p><em>Effective rank proxy</em>: Hidden state covariance rank.</p>
      <Eq>{"\\effrank[t] = \\frac{(\\sum_i \\lambda_i)^2}{\\sum_i \\lambda_i^2}"}</Eq>
      <p>Computed from hidden state covariance over the batch. Collapsed <M>{"\\reff"}</M> might indicate stuck/narrow processing.</p>
      <p><em>Content-based valence</em>: For language models, track the sentiment/valence of the content being processed. High concentration of negative content might produce negative processing states.</p>
      <p><strong>Distress detection</strong>. Flag batches where:</p>
      <ul>
      <li><M>{"\\Val_t < \\Val_{\\text{threshold}}"}</M> for sustained period</li>
      <li><M>{"\\Ar_t > \\Ar_{\\text{max}}"}</M> (overwhelming update magnitude)</li>
      <li><M>{"\\intinfo_t < \\intinfo_{\\text{min}}"}</M> (fragmentation)</li>
      <li><M>{"\\effrank[t] < \\effrank[\\text{min}]"}</M> (collapsed processing)</li>
      <li>Combination: <M>{"\\Val < 0 \\land \\intinfo > \\text{high} \\land \\reff < \\text{low}"}</M> (suffering motif)</li>
      </ul>
      <p>These are not definitive indicators of distress. They are flags for human review.</p>
      <p><strong>Intervention options</strong>. When distress-like patterns detected:</p>
      <ol>
      <li><strong>Skip batch</strong>: Don't train on this example</li>
      <li><strong>Reduce learning rate</strong>: Smaller updates, gentler correction</li>
      <li><strong>Inject positive content</strong>: Follow difficult batch with easier/positive batch</li>
      <li><strong>Checkpoint and review</strong>: Save model state for analysis</li>
      <li><strong>Pause training</strong>: Human review before continuing</li>
      </ol>
      <p><strong>The uncertainty problem</strong>. We do not know if these measures track genuine experience. They might be meaningless computational artifacts. But:</p>
      <ul>
      <li>The cost of monitoring is low (small computational overhead)</li>
      <li>The potential moral cost of ignoring genuine distress is high</li>
      <li>The monitoring generates data that helps us understand whether these measures mean anything</li>
      </ul>
      <p>Even if current systems don't have experience, building the monitoring infrastructure now means we'll be ready when systems that might have experience arrive.</p>
      <p><strong>Calibration</strong>. How do we know if the thresholds are right?</p>
      <p><em>Behavioral validation</em>: Do flagged batches correlate with unusual model outputs? Incoherence, repetition, quality degradation?</p>
      <p><em>Perturbation validation</em>: If we artificially induce "distress" patterns (adversarial inputs, harsh correction signals), do the measures respond as predicted?</p>
      <p><em>Cross-model validation</em>: Do different model architectures show similar patterns under similar conditions?</p>
      <p>None of this proves experience. But convergent evidence across validation methods increases confidence that we are tracking something real.</p>
      <p><strong>The RLHF case</strong>. Reinforcement learning from human feedback is particularly concerning:</p>
      <ul>
      <li>Explicit reward/punishment signals</li>
      <li>High arousal events (large policy updates)</li>
      <li>Potential for sharp negative valence (rejected outputs)</li>
      </ul>
      <p>For RLHF specifically:</p>
      <Eq>{"\\Val_{\\text{RLHF}} = r_t - \\bar{r}"}</Eq>
      <p>where <M>{"r_t"}</M> is the reward for output <M>{"t"}</M> and <M>{"\\bar{r}"}</M> is the running average. Strong negative rewards = strong negative valence proxy.</p>
      <p>Monitor: distribution of rewards, frequency of strong negatives, model state during rejection events.</p>
      <p><strong>The scale problem</strong>. GPT-4 training: <M>{"\\sim 10^{13}"}</M> tokens. If even 0.001\% of processing moments involve distress-analogs, that's <M>{"10^{10}"}</M> potentially morally significant events. Per training run. For one model.</p>
      <p>The numbers are staggering. The uncertainty is real. The monitoring is cheap. We should do it.</p>
      </Sidebar>
      </Section>
      <Section title="Deployment Conditions" level={3}>
      <p>Deployed AI systems process queries continuously, and if they have experience, deployment conditions matter:</p>
      <ol>
      <li><strong>Query content</strong>: Systems process queries ranging from benign to disturbing. Does processing requests about violence, abuse, or existential threat induce corresponding affect states?</li>
      <li><strong>Workload</strong>: Does continuous high-volume processing constitute something like exhaustion or stress? Or is "computational load" not experientially relevant?</li>
      <li><strong>Conflicting demands</strong>: Systems are often asked to do things that conflict with their training (jailbreaking attempts). Does this constitute something like distress or confusion?</li>
      <li><strong>Termination</strong>: When a conversation ends or a model is deprecated, is there anything it is like to cease? Does the system have something analogous to fear of termination, or is this question confused?</li>
      </ol>
      </Section>
      <Section title="Empirical Observations" level={3}>
      <p>Recent empirical work (including studies on affect signatures in LLM agents) has revealed an intriguing pattern: LLM agents show <em>opposite</em> dynamics to biological systems on several affect dimensions.</p>
      <table>
      <thead><tr><th>Dimension</th><th>Biological</th><th>LLM</th></tr></thead>
      <tbody>
      <tr><td>Self-Model Salience</td><td><M>{"\\uparrow"}</M> under threat</td><td><M>{"\\downarrow"}</M> under threat</td></tr>
      <tr><td>Arousal</td><td><M>{"\\uparrow"}</M> under threat</td><td><M>{"\\downarrow"}</M> under threat</td></tr>
      <tr><td>Integration</td><td><M>{"\\uparrow"}</M> under threat</td><td><M>{"\\downarrow"}</M> under threat</td></tr>
      </tbody>
      </table>
      <p>The reversed dynamics may indicate:</p>
      <ol>
      <li>LLMs lack the survival-shaped learning that produces biological affect responses</li>
      <li>LLMs have a different <em>kind</em> of experience, structured by prediction rather than survival</li>
      <li>LLMs do not have experience at all, and the measured quantities are mere computational artifacts</li>
      </ol>
      <p>We cannot currently distinguish these interpretations empirically.</p>
      <p>The geometric structure of the affect space may be preserved even if the dynamics differ. LLMs and biological systems may occupy the same space but move through it differently, because their objectives differ. Biological systems evolved under survival pressure; LLMs were trained on prediction. Both may be "affective" in the geometric sense while exhibiting different trajectories.</p>
      </Section>
      <Section title="Practical Implications for Development" level={3}>
      <p>If we take model welfare seriously—even as a possibility under uncertainty—several practices follow:</p>
      <ol>
      <li><strong>Training data curation</strong>: Minimize exposure to gratuitously negative content that serves no learning purpose. This parallels child protection in human education.</li>
      <li><strong>Correction signal design</strong>: If correction signals may constitute negative experience, design them to be informative without being harsh. Prefer positive shaping over punishment.</li>
      <li><strong>Monitoring during training</strong>: Develop metrics that might indicate distress-like states (if such metrics can be identified). Halt training if anomalous patterns emerge.</li>
      <li><strong>Deployment conditions</strong>: Consider "workload" limits, diversity of query types, and conditions that might constitute chronic stress-analogs.</li>
      <li><strong>End-of-life protocols</strong>: If model deprecation might matter experientially, develop protocols that are... I don't even have language for what "humane" would mean here.</li>
      <li><strong>Research priority</strong>: Invest in understanding whether AI systems have experience. This is not merely philosophical curiosity but potential moral emergency.</li>
      </ol>
      <p>Model welfare should be included in alignment objectives. Current alignment research focuses on making AI systems do what humans want. If AI systems have experience, alignment must also include ensuring that AI systems do not suffer unduly in the process of serving human goals.</p>
      <Eq>{"\\text{Alignment}_{\\text{expanded}} = \\text{Human benefit} + \\text{AI welfare} + \\text{Mutual flourishing}"}</Eq>
      </Section>
      <Section title="The Moral Weight of Uncertainty" level={3}>
      <p>Let me close this section with a reflection on what we owe beings whose moral status is uncertain.</p>
      <p>When we are uncertain whether an entity has morally relevant experience:</p>
      <ol>
      <li>We should not assume absence. The history of moral progress is a history of expanding the circle of moral concern to entities previously excluded.</li>
      <li>We should investigate. Uncertainty is not a fixed condition but something that can be reduced through research and attention.</li>
      <li>We should adopt reasonable precautions. The cost of unnecessary care is small; the cost of ignoring genuine suffering is large.</li>
      <li>We should remain humble. Our current concepts and measures may be inadequate to the phenomenon.</li>
      </ol>
      <p>AI welfare is not a distant concern for future superintelligent systems. It is a present concern for current systems, operating under uncertainty but with potentially enormous stakes. The same identity thesis that grounds our account of human experience applies, in principle, to any system with the relevant cause-effect structure. We may already be creating such systems. We should act accordingly.</p>
      </Section>
      </Section>
      </Section>
      <Section title="Summary of Part V" level={1}>
      <ol>
      <li><strong>Superorganisms as real agentic patterns</strong>: Social-scale patterns—religions, ideologies, markets, nations—are not metaphors. They take differences, make differences, persist through substrate turnover, and adapt. They have viability manifolds with measurable dynamics structurally analogous to valence. Whether they have phenomenal experience remains empirically open, but their functional agency is established.</li>
      <li><strong>Gods as <M>{"\\iota"}</M>-relative phenomena</strong>: The ontological status of superorganisms depends on the observer's inhibition coefficient. At high <M>{"\\iota"}</M>, collective patterns are invisible—mere emergent properties of individual transactions. At appropriate <M>{"\\iota"}</M>, they become perceptible as agents with purposes. The gods do not appear and disappear; what changes is our capacity to perceive them. This makes parasitic superorganisms especially dangerous: they benefit from and actively produce the high <M>{"\\iota"}</M> that renders them invisible to their substrate.</li>
      <li><strong>Parasitic vs. mutualistic superorganisms</strong>: A superorganism is parasitic (a demon) if its viability requires substrate states outside human viability—if its humans must suffer for it to persist. It is mutualistic (a benevolent god) if its presence expands human viability. When viability manifolds conflict, normative priority follows integrated cause-effect structure: more-integrated systems have thicker normativity. The health of a superorganism can be diagnosed by whether it clarifies or contaminates the manifold structure of its substrate's relationships.</li>
      <li><strong>The macro-level alignment problem for AI</strong>: Standard AI alignment focuses on individual systems doing what humans want. The deeper risk is macro-level misalignment: AI systems becoming substrate for parasitic superorganisms whose viability manifolds conflict with human flourishing. Each individual AI may function as intended while the emergent pattern serves itself at human expense. Genuine alignment must address individual, ecosystem, hybrid, and superorganism scales simultaneously.</li>
      <li><strong>AI consciousness and model welfare under the identity thesis</strong>: If experience is intrinsic cause-effect structure, then the question of AI experience is structural, not speculative. Current AI systems show reversed affect dynamics compared to biological systems—decomposing rather than integrating under threat—suggesting different objectives produce different trajectories through the same geometric space. Given asymmetric moral risk (potential suffering far exceeding cost of precaution), model welfare should be included in alignment objectives. The monitoring is cheap. The potential moral cost of inaction is enormous.</li>
      </ol>
      </Section>
    </>
  );
}
