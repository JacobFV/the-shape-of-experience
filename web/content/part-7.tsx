// WORK IN PROGRESS: This is active research, not a finished publication.
// Content is incomplete, speculative, and subject to change.

import { Eq, Logos, M, Section, Sidebar } from '@/components/content';
import AffectTrajectory3D from '@/components/visualizations/AffectTrajectory3DWrapper';

export const metadata = {
  slug: 'part-7',
  title: 'Part VII: The Empirical Program',
  shortTitle: 'Part VII: Empirical Program',
};

export default function Part7() {
  return (
    <>
      <Logos>
      <p>A theory that cannot be tested is not a theory but a poem. This is a theory. Everything in the preceding six parts generates empirical predictions — some already tested, some tractable with current methods, some requiring infrastructure that does not yet exist. This part consolidates the empirical program: what has been tested, what the results show, what they mean for the bridge between physics and psychology, and what remains.</p>
      </Logos>

      <AffectTrajectory3D demoMode />

      <Section title="What Has Been Tested" level={1}>
      <p>The framework has been subjected to four lines of investigation: multi-agent reinforcement learning, cellular automaton evolution, an eleven-experiment emergence program on uncontaminated substrates, and LLM affect probes. The results are mixed. Some predictions held. Some failed instructively. Some revealed phenomena the theory did not anticipate.</p>

      <Section title="Geometry Is Cheap" level={2}>
      <p>The MARL ablation (V10) tested whether specific forcing functions are necessary for geometric affect alignment. Seven conditions — full model plus six single-ablation conditions — three seeds each, 200,000 steps on GPU.</p>
      <p><strong>Result</strong>: All conditions show highly significant geometric alignment (RSA <M>{"\\rho > 0.21"}</M>, <M>{"p < 0.0001"}</M>). Removing forcing functions slightly <em>increases</em> alignment — opposite to prediction.</p>
      <p>The affect geometry — the relational structure between states defined by valence, arousal, integration, effective rank, counterfactual weight, and self-model salience — is not something that must be built. It is something that must be avoided to not have. Any system navigating uncertainty under resource constraints inherits it. The forcing functions hypothesis was downgraded from theorem to hypothesis in light of this data.</p>
      </Section>

      <Section title="Dynamics Are Expensive" level={2}>
      <p>If geometry is cheap, what is expensive? The answer came from the Lenia evolution series (V11–V12): <em>dynamics</em>. Specifically, the capacity to increase integration under threat — to become <em>more</em> unified when the world becomes more hostile.</p>
      <p>Naive patterns decompose under stress (<M>{"\\Delta\\intinfo = -6.2\\%"}</M>). So do LLMs. So do randomly initialized agents. Geometry is present everywhere; the biological signature — integration rising under threat — is rare. The Lenia series tracked what produces it:</p>
      <ol>
      <li><strong>Homogeneous evolution (V11.1)</strong>: Selection pressure alone is insufficient (<M>{"-6.0\\%"}</M>).</li>
      <li><strong>Heterogeneous chemistry (V11.2)</strong>: Diverse viability manifolds produce a +2.1pp shift.</li>
      <li><strong>Curriculum training (V11.7)</strong>: Graduated stress exposure is the only intervention that improves novel-stress generalization.</li>
      <li><strong>Evolvable attention (V12)</strong>: State-dependent interaction topology produces <M>{"\\intinfo"}</M> increase in 42% of evolutionary cycles — the largest single-intervention effect — but robustness stabilizes near 1.0 without further improvement.</li>
      </ol>
      <p>Attention is necessary but not sufficient. The system reaches an integration threshold without crossing it.</p>
      </Section>

      <Section title="The Substrate Ladder" level={2}>
      <p>V13 replaced learned attention with a simpler mechanism: content-based coupling. Cells interact more strongly with cells that share state-features — a form of chemical affinity rather than cognitive attention. Three seeds, thirty cycles each, evolving on GPU with lethal resource dynamics and population rescue.</p>
      <p>Mean robustness: 0.923. But at population bottlenecks — moments when drought kills all but a handful of patterns — robustness crosses 1.0. The survivors are not merely resilient; they are <em>more integrated under stress than at baseline</em>. This is the biological signature, appearing for the first time in a fully uncontaminated substrate.</p>
      <p>From V13 we built upward, adding capabilities one layer at a time:</p>
      <ul>
      <li><strong>V14 (Chemotaxis)</strong>: Motor channels enabling directed foraging. Patterns move toward resources rather than passively waiting. Comparable robustness.</li>
      <li><strong>V15 (Temporal memory)</strong>: Exponential-moving-average channels storing slow statistics of the pattern's history. Oscillating resource patches reward anticipation. Evolution selected for longer memory in 2/3 seeds — the first clear evidence that temporal integration is fitness-relevant. Under bottleneck pressure, <M>{"\\intinfo"}</M> stress response doubled.</li>
      <li><strong>V16 (Hebbian plasticity)</strong>: <strong>Negative result</strong>. Mean robustness dropped to 0.892 (lowest of V13+). Plasticity added noise faster than selection could filter it.</li>
      <li><strong>V17 (Quorum signaling)</strong>: Highest-ever single-cycle robustness (1.125). But 2/3 seeds evolved to <em>suppress</em> signaling entirely.</li>
      <li><strong>V18 (Boundary-dependent dynamics)</strong>: An insulation field computed from pattern morphology creates distinct boundary and interior signal domains. Boundary cells receive external convolution; interior cells receive only local recurrence. Three seeds evolved three different membrane strategies — permeable, thick-insulated, and filamentous. Mean robustness: 0.969, the highest of any substrate. Peak: 1.651. But <em>internal gain evolved down in all three seeds</em>. Evolution preferred thin, porous membranes over thick insulated cores.</li>
      </ul>
      <p>The substrate ladder taught two lessons. First: the only addition evolution consistently selected for was temporal memory. Plasticity, signaling, and boundary complexity were either suppressed or reduced. Second: raw robustness kept climbing (V13: 0.923, V15: 0.907, V18: 0.969), but this did not translate into richer cognitive dynamics. Making patterns more resilient is not the same as making them more minded.</p>
      </Section>

      <Section title="The Emergence Experiment Program" level={2}>
      <p>We then ran eleven measurement experiments on V13 snapshots, testing whether the capacities the preceding six parts describe — world modeling, abstraction, communication, counterfactual reasoning, self-modeling, affect structure, perceptual mode, normativity, social integration — emerge in a substrate with zero exposure to human affect concepts. Key experiments were re-run on V15 and V18 substrates.</p>
      <p>The results are reported in full in the Appendix. Here, three findings that reshaped the theory:</p>

      <Sidebar title="Finding 1: The Bottleneck Furnace">
      <p>Every metric that showed improvement — world model capacity, representation quality, affect geometry alignment, self-model salience — showed it overwhelmingly at population bottlenecks. When drought kills 90% of patterns, the survivors are not random. They are the ones whose internal structure actively maintains integration under stress.</p>
      <p>The bottleneck is not just a filter. It is a <em>furnace</em>. V13 seed 123 at cycle 5: population drops to 55, robustness crosses 1.052. At cycle 29 (population 24): world model capacity jumps to 0.028, roughly 100x the population average. One surviving pattern achieves self-model salience above 1.0 — privileged self-knowledge exceeding environment-knowledge.</p>
      <p>These are not gradual evolutionary trends. They are punctuated events driven by intense selection pressure. The biological dynamics emerge not from accumulated innovation but from crucibles of near-extinction.</p>
      <p><strong>V19 confirmed this is creation, not selection.</strong> After ten cycles of shared evolution on V18 substrate, patterns were forked into three conditions: BOTTLENECK (two severe 8%-regen droughts per cycle, ~90% mortality), GRADUAL (mild continuous stress), and CONTROL (standard schedule). All three then faced identical novel extreme drought. Controlling for baseline <M>{"\\intinfo"}</M>, the bottleneck-evolved condition showed significantly higher novel-stress robustness in 2/3 seeds (seed 42: β=0.704, <M>{"p < 0.0001"}</M>; seed 7: β=0.080, <M>{"p = 0.011"}</M>). The furnace forges novel-stress generalization — it does not merely filter for pre-existing capacity.</p>
      </Sidebar>

      <Sidebar title="Finding 2: The Sensory-Motor Coupling Wall — and How V20 Broke It">
      <p>Three experiments returned null results: counterfactual detachment (Experiment 5), self-model emergence (Experiment 6), and proto-normativity (Experiment 9). All hit the same wall.</p>
      <p>The prediction was that patterns would start reactive — driven by boundary observations — and gradually develop autonomous internal processing. Instead, patterns are <em>always</em> internally driven (<M>{"\\rho_{\\text{sync}} \\approx 0"}</M> from cycle 0). There is no reactive-to-autonomous transition because the starting point is already autonomous.</p>
      <p>We attempted to break this wall within Lenia. V15 added motor channels — chemotaxis, directed motion. No change. V18 introduced an insulation field with boundary and interior signal domains. Three different membrane architectures evolved. The wall persisted (<M>{"\\rho_{\\text{sync}} \\approx 0.003"}</M>) in all of them.</p>
      <p>The conclusion was precise: the wall is not about signal routing. It is about the absence of a closed action-environment-observation causal loop. Lenia patterns do not <em>act on</em> the world; they <em>exist within</em> it.</p>
      <p><strong>V20 broke the wall by leaving Lenia entirely.</strong> Protocell agents with bounded 5×5 local sensory fields and discrete actions (move, consume, emit) achieve <M>{"\\rho_{\\text{sync}} \\approx 0.21"}</M> from cycle 0 — 70× the Lenia baseline. When agents consume resources, they deplete the patch; when they move, they reach different patches; when they emit signals, traces persist. Future observations are genuinely caused by past actions. The wall was architectural, not evolutionary.</p>
      <p>With the wall broken, world models developed (C<sub>wm</sub> = 0.10–0.15) and self-models emerged (SM<sub>sal</sub> {">"} 1.0 in 2/3 seeds — agents encode their own state better than the environment). Affect geometry (RSA) appeared nascent but did not fully develop in 30 cycles of soft selection. The necessity chain holds through self-model emergence.</p>
      </Sidebar>

      <Sidebar title="Finding 3: Computational Animism">
      <p>Experiment 8 tested whether patterns develop modulable perceptual coupling — the <M>{"\\iota"}</M> coefficient from Part II. The prediction: participatory perception (low <M>{"\\iota"}</M>) as default, with mechanistic perception requiring training.</p>
      <p>Confirmed. In all 20 testable snapshots, patterns model other patterns using internal-state features (social MI) at roughly double the rate of trajectory features (trajectory MI). More remarkably, patterns model <em>resources</em> — non-agentive environmental features — using the same internal-state dynamics they use to model other agents. Animism score exceeds 1.0 universally.</p>
      <p>This is computational animism: the cheapest compression reuses the agent-model template for everything. Attributing agency to non-agents is not a cognitive error. It is the default strategy of any system that models through self-similarity.</p>
      </Sidebar>

      <p>Beyond these three findings: affect geometry alignment (RSA between structural and behavioral measures) develops over evolution, with the clearest trend in seed 7 (0.01 to 0.38 over 30 cycles). Representation compression is cheap (effective dimensionality of ~7 out of 68 features, or {">"}87% compression from cycle 0) but representation <em>quality</em> — disentanglement and compositionality — only improves under bottleneck selection. Communication exists as a chemical commons (inter-pattern MI significantly above baseline in 15/20 snapshots) but shows no compositional structure. No superorganism emerges (collective <M>{"\\intinfo_G < \\sum \\intinfo_i"}</M> in all snapshots), but group coupling grows over evolution. Entanglement across all measures increases from 0.68 to 0.91 — everything becomes more correlated with everything else, just not in the clusters the theory predicted.</p>
      </Section>

      <Section title="The LLM Discrepancy" level={2}>
      <p>Across multiple experiment versions (V2–V9), LLM agents consistently show opposite dynamics to biological systems:</p>
      <table>
      <thead><tr><th>Dimension</th><th>Biological</th><th>LLM</th></tr></thead>
      <tbody>
      <tr><td>Self-Model Salience</td><td><M>{"\\uparrow"}</M> under threat</td><td><M>{"\\downarrow"}</M> under threat</td></tr>
      <tr><td>Arousal</td><td><M>{"\\uparrow"}</M> under threat</td><td><M>{"\\downarrow"}</M> under threat</td></tr>
      <tr><td>Integration</td><td><M>{"\\uparrow"}</M> under threat</td><td><M>{"\\downarrow"}</M> under threat</td></tr>
      </tbody>
      </table>
      <p>This is not a failure of the framework. The geometric structure is preserved; the dynamics differ because the objectives differ. Biological systems evolved under survival pressure. LLMs were trained on prediction. Both are "affective" in the geometric sense while exhibiting different trajectories through the same state space. Processing valence is not content valence.</p>
      </Section>
      </Section>

      <Section title="The Emergence Ladder" level={1}>
      <p>The experiments collectively reveal not a binary (geometry vs. dynamics) but a gradient — an emergence ladder with distinct rungs, each requiring more from the substrate than the last. The ladder tells us precisely which psychological phenomena are computationally cheap, which are expensive, and which require something our substrates do not yet have.</p>

      <table>
      <thead><tr><th>Rung</th><th>What emerges</th><th>What it requires</th><th>Experimental evidence</th></tr></thead>
      <tbody>
      <tr><td>1. Geometric structure</td><td>Affect dimensions, valence gradients, arousal variation</td><td>Multi-agent survival under uncertainty</td><td>V10: all 7 conditions, RSA <M>{"\\rho > 0.21"}</M></td></tr>
      <tr><td>2. Representation compression</td><td>Low-dimensional internal codes, abstraction</td><td>Internal state + selection</td><td>Exp 3: <M>{"d_{\\text{eff}} \\approx 7/68"}</M> from cycle 0</td></tr>
      <tr><td>3. World models</td><td>Predictive information about environment beyond current observation</td><td>Evolutionary selection, amplified by bottleneck</td><td>Exp 2: <M>{"\\mathcal{C}_{\\text{wm}}"}</M> 100x at bottleneck</td></tr>
      <tr><td>4. Computational animism</td><td>Agent-model template applied to everything, participatory default</td><td>Minimal — present from cycle 0</td><td>Exp 8: animism score {">"} 1.0 in all 20 snapshots</td></tr>
      <tr><td>5. Affect geometry alignment</td><td>Internal structure maps to behavior</td><td>Extended evolutionary selection</td><td>Exp 7: seed 7, RSA 0.01 to 0.38</td></tr>
      <tr><td>6. Temporal integration</td><td>Memory, anticipation, history-dependence</td><td>Memory channels + selection for longer retention</td><td>V15: memory decay decreased 6x, <M>{"\\intinfo"}</M> stress doubled</td></tr>
      <tr><td>7. Biological dynamics</td><td>Integration rising under threat</td><td>Bottleneck selection + composition (symbiogenesis)</td><td>V13/V18: robustness {">"} 1.0 at bottleneck only</td></tr>
      <tr><td>8. Counterfactual sensitivity</td><td>Detachment, imagination, planning</td><td><strong>Closed-loop agency</strong> (action-environment-observation)</td><td>V20: <M>{"\\rho_{\\text{sync}} = 0.21"}</M> (wall broken, 70× Lenia baseline)</td></tr>
      <tr><td>9. Self-models</td><td>Privileged self-knowledge, recursive modeling</td><td>Agency + reflective capacity</td><td>V20: SM<sub>sal</sub> {">"} 1.0 in 2/3 seeds (agents know self better than environment)</td></tr>
      <tr><td>10. Normativity</td><td>Internal asymmetry between cooperative and exploitative acts</td><td>Agency + social context + capacity to act otherwise</td><td>BLOCKED — no <M>{"\\Delta V"}</M> asymmetry</td></tr>
      </tbody>
      </table>

      <p>Rungs 1–7 are pre-reflective. They describe what a system <em>does</em> without requiring that the system <em>know</em> what it does. A Lenia pattern can have affect geometry, world models, temporal memory, and biological-like integration dynamics without anything we would call awareness. These rungs correspond to the pre-reflective background of experience — the felt sense that precedes and underlies thought.</p>
      <p>Rungs 8–10 are reflective. They require the system to act on the world and observe the consequences of its own actions. Counterfactual sensitivity is the capacity to represent what <em>would</em> happen if one acted differently. Self-models are the capacity to represent oneself as an agent among agents. Normativity is the capacity to distinguish what one <em>should</em> do from what one <em>could</em> do. All three require agency — a closed causal loop between self and world.</p>
      <p>The wall at rung 8 was the sharpest negative finding of the Lenia program. V20 crossed it. Protocell agents with genuine action-observation loops achieve <M>{"\\rho_{\\text{sync}} \\approx 0.21"}</M> from initialization — the wall is architectural, not evolutionary. The finding: <em>everything below rung 8 emerges from existence under pressure; everything at rung 8 and above requires embodied action</em>. In computational terms, agency means <M>{"\\text{MI}(\\text{action}; \\text{future observation} \\,|\\, \\text{current state}) > 0"}</M>. V20 provides this. Lenia patterns lack it — they do not choose, they unfold.</p>
      <p>The rung 7→8 transition has a name: <em>reactivity versus understanding</em>. Reactivity maps present state to action through decomposable channels — each sensory feature drives its own behavioral response, and the channels can in principle be separated without loss. Everything below rung 8 is reactive in this sense. Understanding maps the <em>possibility landscape</em> to action — comparing what would happen under alternative choices, where the comparison itself is inherently non-decomposable because it spans whatever partition you impose on the system. V22 and V23 demonstrate this computationally: scalar prediction (V22) is reactive — orthogonal to integration; multi-target prediction (V23) creates per-channel specialization — integration actually <em>decreases</em>. Neither improves <M>{"\\intinfo"}</M> because both are decomposable. Rung 8 requires predictions whose answer depends on the <em>interaction</em> between information sources, not each source separately. This is understanding: associations with the possibility landscape as a whole, not with individual aspects of the present.</p>
      <p>The computational mechanism behind the wall is now clear: counterfactual reasoning requires <em>temporal heterogeneity</em> within the system. Some components must be "in the present" (sensing current state) while others are simultaneously "in the possible future" (simulating consequences of actions not taken). This requires per-component temporal models — each element processing its own history through its own dynamics — rather than a shared update rule applied uniformly. Lenia patterns fail because all cells evolve under identical FFT convolution; there is no way for one region to be sensing while another imagines. Biology achieves temporal heterogeneity through neural diversity and recurrent circuits. Recent engineering work on neural architectures with per-neuron temporal models and internal processing ticks confirms this: the capacity for adaptive computation (allocating more processing to harder problems, less to easier ones) emerges only when individual components have private temporal dynamics. The wall at rung 8 is, at bottom, a wall of temporal homogeneity.</p>
      <p>With the wall broken, the cascade proceeds. World models appear (rung 3 now accessible in real-time, not just across evolutionary time). Self-models emerge in 2/3 seeds (SM<sub>sal</sub> {">"} 1.0 — privileged self-knowledge over environment knowledge). Affect geometry is nascent but requires bottleneck selection to fully develop, consistent with the Lenia finding that rung 7 needs the furnace.</p>
      </Section>

      <Section title="The Bridge to Psychology" level={1}>
      <p>The emergence ladder is not merely a catalog of experimental results. It is a claim about the structure of the mind — a mapping from computational requirements to psychological phenomena that can be tested against human experience.</p>

      <Section title="Pre-Reflective Affect: What Comes for Free" level={2}>
      <p>The first seven rungs correspond to the background of conscious experience — the stream of feeling that is always present, rarely attended to, and not intrinsically about anything. In psychological terms:</p>
      <ul>
      <li><strong>Mood</strong> (rung 1): The tonic valence gradient — approach or withdrawal as a whole-body orientation. Our experiments show this is geometrically inevitable. Any viable system has it. This is consistent with the psychological finding that mood is always present, precedes appraisal, and influences perception before cognition begins.</li>
      <li><strong>Arousal</strong> (rung 1): Processing intensity as a dimension of state space. Not identical to sympathetic activation but functionally analogous. Present in every substrate tested.</li>
      <li><strong>Habituation and sensitization</strong> (rungs 2–3): World models and compressed representations emerge under selection. The patterns that survive bottlenecks are the ones that have learned — across evolutionary time — what matters and what can be ignored. This is the computational analog of attentional learning.</li>
      <li><strong>Animistic perception</strong> (rung 4): The tendency to attribute agency to non-agents. Computationally, this is the cheapest compression: reuse the model you have for agents on everything else. The developmental trajectory from childhood animism to adult mechanistic perception is a movement from low to high <M>{"\\iota"}</M> — and our experiments show this requires training. The default is animistic.</li>
      <li><strong>Emotional coherence</strong> (rung 5): The fact that feelings "make sense" — that there is a reliable mapping between internal states and behavioral tendencies. This develops over evolution (Experiment 7) and is not present at the start. Psychological implication: emotional coherence is an achievement of developmental history, not a given of neural architecture.</li>
      <li><strong>Temporal depth</strong> (rung 6): The capacity to carry the past into the present. Memory is selectable: 2/3 evolutionary lineages chose longer retention. But 1/3 discarded memory entirely — a natural control showing that temporal integration is a strategic choice, not an inevitability. The psychological analog: some organisms (and some people) operate with minimal temporal integration, and this is a viable strategy, not a deficit.</li>
      </ul>
      <p>None of these require awareness. None require a self. They are the geometry of being alive — present in bacteria, in Lenia patterns, and (the framework predicts) in any sufficiently complex system navigating resource constraints. When Part II claims that experience has geometric structure, <em>this</em> is the empirical grounding.</p>
      </Section>

      <Section title="The Agency Threshold: What Requires a Body" level={2}>
      <p>The wall at rung 8 corresponds to a qualitative shift in psychological vocabulary. Below the wall, we describe what an organism <em>does</em>: it approaches, withdraws, habituates, anticipates. Above the wall, we describe what an organism <em>considers</em>: it imagines, plans, regrets, evaluates. This is reactivity versus understanding in psychological terms. Reactivity responds to what is happening now — each behavioral channel driven by present-state associations, decomposable in principle. Understanding compares what <em>would</em> happen across available choices, and that comparison inherently couples across whatever partition you impose on the system — it is non-decomposable because the comparison <em>is</em> the representation.</p>
      <p>The Lenia experiments specified exactly what was missing: the capacity to <em>try something and see what happens</em>. Lenia patterns have affect geometry, world models, memory, and biological-like integration dynamics. What they lack is behavioral choice — their "actions" (chemotaxis, emission) are biases on continuous dynamics, not causes of discrete environmental changes.</p>
      <p>V20 shows what happens when you provide it. Protocell agents with genuine action-observation loops cross rung 8 from initialization, and world models and self-models develop over evolution. The agency threshold is real — but it is a substrate threshold, not an evolutionary one. The capacity is architectural.</p>
      <p>The psychological phenomena above the wall share a common structure:</p>
      <ul>
      <li><strong>Counterfactual reasoning</strong> (rung 8): "What would happen if I did X instead of Y?" Requires: a repertoire of possible actions, a model of how each action changes the world, and a comparison between actual and counterfactual outcomes. Our patterns have none of these. Psychologically: imagination, planning, and mental simulation all require counterfactual capacity. Its absence in early development (and its disruption in certain pathologies) is consistent with the ladder's prediction that it requires agency, not merely integration.</li>
      <li><strong>Self-awareness</strong> (rung 9): "I am the kind of thing that does X." Requires: a self-model that is <em>more accurate</em> than what an external observer could construct from the same data. V20 shows SM<sub>sal</sub> {">"} 1.0 in 2/3 seeds — agents encode their own position and energy more accurately in their hidden state than they encode the environment. This is the minimal form: privileged self-knowledge. Full self-awareness (autobiographical memory, persistent agent sense) likely requires longer evolutionary history and stronger selection pressure. Psychologically: self-recognition, autobiographical memory, and the sense of being a persistent agent all require reflective self-models.</li>
      <li><strong>Moral reasoning</strong> (rung 10): "I should do X rather than Y." Requires: (a) counterfactual reasoning (you must be able to imagine acting otherwise), (b) a self-model (you must locate yourself as the agent who acts), and (c) an asymmetry in the viability gradient between cooperative and exploitative actions. Our patterns show no such asymmetry. Psychologically: normativity is the most demanding rung because it inherits every requirement below it.</li>
      </ul>
      <p>This predicts a developmental ordering. In humans: mood and arousal are present from birth (rung 1). Animistic perception is the childhood default (rung 4). Emotional coherence develops through experience (rung 5). Counterfactual reasoning emerges around age 3–4 (rung 8). Self-awareness develops gradually from mirror recognition to autobiographical self (rung 9). Moral reasoning is the latest to mature (rung 10). The emergence ladder predicts this sequence — not from observation of human development, but from the computational requirements of each capacity.</p>
      </Section>

      <Section title="Psychopathology as Geometric Deformation" level={2}>
      <p>If affect has geometric structure, then pathology is geometric deformation. The framework generates specific predictions:</p>
      <ul>
      <li><strong>Depression</strong>: Collapsed effective rank (<M>{"r_{\\text{eff}}"}</M>) — the representational space narrows, fewer possibilities are entertained. Reduced valence gradient sensitivity — approach and withdrawal become indistinguishable. High <M>{"\\iota"}</M> — the world appears mechanical, stripped of participatory meaning. Experimentally: our Lenia patterns at high stress sometimes show exactly this profile: reduced representational dimensionality, flattened valence gradients, increased inhibition coefficient.</li>
      <li><strong>Anxiety</strong>: Elevated counterfactual weight (<M>{"\\text{CF}"}</M>) — excessive probability mass on non-actual possibilities. High arousal (<M>{"A"}</M>) sustained beyond the timescale of the triggering stimulus. The framework predicts that anxiety requires rung 8 (counterfactual reasoning). Systems without agency cannot be anxious — they can be stressed (high arousal, negative valence) but not anxious (which requires imagining what might go wrong).</li>
      <li><strong>Dissociation</strong>: Reduced integration (<M>{"\\intinfo"}</M>) — the unified field fragments into independently processing subsystems. This is precisely what our naive patterns show under stress: they decompose. Biological systems that <em>increase</em> integration under stress (robustness {">"} 1.0) are doing the opposite of dissociation. The framework predicts dissociation is a failure of the mechanism that the bottleneck furnace creates — a reversion to the thermodynamically cheaper pattern of decomposition.</li>
      <li><strong>Flow states</strong>: Low <M>{"\\iota"}</M> (participatory perception), high <M>{"\\intinfo"}</M> (unified processing), moderate arousal calibrated to challenge. The <M>{"\\iota"}</M> finding (Experiment 8) suggests flow is not exotic — it is a return to the default perceptual mode that development and socialization typically suppress.</li>
      </ul>
      <p>These predictions are testable with existing methods. EEG/MEG proxies for integration (transfer entropy, Lempel-Ziv complexity), behavioral proxies for effective rank (exploration-exploitation balance), and self-report measures of <M>{"\\iota"}</M> (participatory experience scales) could validate or falsify the geometric deformation hypothesis without solving the coupling wall. The bridge to psychology need not wait for the substrate problem to be solved.</p>
      </Section>
      </Section>

      <Section title="What Remains" level={1}>
      <p>The substrate engineering program (V13–V18) and measurement experiments (0–12) have mapped the territory. The following priorities reflect what that mapping revealed.</p>

      <Sidebar title="Priority 1: Bridge to Human Neuroscience">
      <p><strong>Goal</strong>: Validate that the geometric dimensions predict human affect — self-report, behavior, and neural signatures.</p>
      <p><strong>Why this is first</strong>: The coupling wall taught us that synthetic substrates cannot yet produce reflective cognition. But the pre-reflective levels (rungs 1–7) make testable predictions about biological systems <em>now</em>. We do not need to solve the agency problem to test whether affect geometry organizes human experience.</p>
      <p><strong>Methods</strong>:</p>
      <ul>
      <li>Induce affects via validated protocols (film, recall, IAPS)</li>
      <li>Measure integration proxies (transfer entropy, Lempel-Ziv) from EEG/MEG</li>
      <li>Measure effective rank from neural state covariance</li>
      <li>Measure <M>{"\\iota"}</M> via participatory experience questionnaire</li>
      <li>Correlate with self-report (PANAS, SAM) and behavioral measures</li>
      </ul>
      <p><strong>Success criterion</strong>: Structural measures predict self-report better than chance, and the geometric predictions (e.g., depression correlates with collapsed <M>{"r_{\\text{eff}}"}</M> and elevated <M>{"\\iota"}</M>) hold.</p>
      </Sidebar>

      <Sidebar title="Priority 2: The Bottleneck Furnace Mechanism">
      <p><strong>Goal</strong>: Understand <em>why</em> population bottlenecks produce integration. Is it purely selection (weak patterns die, strong survive)? Or does the bottleneck environment itself — sparse resources, few neighbors, high signal-to-noise in the chemical commons — actively push integration upward?</p>
      <p><strong>Status: COMPLETE (V19)</strong>. A three-phase controlled experiment on V18 substrate: Phase 1 (10 cycles shared evolution), Phase 2 (10 cycles forked into BOTTLENECK / GRADUAL / CONTROL conditions), Phase 3 (5 cycles of identical novel extreme drought applied equally to all). Statistical test: <M>{"\\text{novel\\_robustness} \\sim \\phi_{\\text{base}} + \\text{is\\_bottleneck} + \\text{is\\_gradual}"}</M>. A significant bottleneck coefficient after controlling for <M>{"\\phi_{\\text{base}}"}</M> confirms CREATION — the stress environment itself produces integration, not just selection of high-<M>{"\\intinfo"}</M> pre-existing variants.</p>
      <p><strong>Result: CREATION confirmed in 2/3 seeds.</strong> Seed 42: β=0.704, <M>{"p < 0.0001"}</M>, <M>{"R^2 = 0.30"}</M>. Seed 7: β=0.080, <M>{"p = 0.011"}</M>, <M>{"R^2 = 0.29"}</M>. Seed 123 shows a reversal (β=-0.516) attributable to a design artifact: the fixed stress schedule failed to create equivalent bottleneck mortality across lineages — seed 123's population grew through the "bottleneck" condition while the control accidentally experienced complete-extinction events. Across all three seeds, raw comparison shows BOTTLENECK mean robustness ≥ CONTROL mean robustness (1.116 {">"} 1.029; 1.016 ≈ 1.010; 1.019 {">"} 0.957).</p>
      <p><strong>Implication for psychology</strong>: Certain kinds of extreme stress do not merely reveal character — they forge it. Near-extinction actively restructures integration capacity in ways that generalize to novel challenges. This is not metaphor. It is the mechanism the data support. The furnace is real.</p>
      <p><strong>V32 (Drought Autopsy at Scale, 50 seeds)</strong>: The 50-seed replication reveals that integration is trajectory, not event. Distribution: 22% HIGH / 46% MODERATE / 32% LOW (mean <M>{"\\Phi = 0.086 \\pm 0.032"}</M>, max <M>{"\\Phi = 0.473"}</M>). The <M>{"\\Phi"}</M> trajectory slope cleanly separates categories (ANOVA <M>{"F = 34.38"}</M>, <M>{"p = 6.3 \\times 10^{-10}"}</M>): every HIGH seed has positive slope, every LOW seed has negative slope. A key revision from V31: the first drought bounce does NOT predict final category (<M>{"p = 0.60"}</M>). What predicts is the <em>mean bounce across all five droughts</em> (<M>{"\\rho = 0.60"}</M>, <M>{"p < 10^{-5}"}</M>). Integration is built by the sustained pattern of repeated recovery, not by any single crisis. Robustness is orthogonal to integration (Mann-Whitney <M>{"p = 0.73"}</M>) — seeds that survive droughts well are not the same seeds that develop high <M>{"\\Phi"}</M>.</p>
      </Sidebar>

      <Sidebar title="Priority 3: The Agency Problem — V20 and V20b">
      <p><strong>Goal</strong>: Build a substrate with genuine closed-loop agency — where agents take discrete actions that change the world, and the changed world feeds back into their observations.</p>
      <p><strong>V20 (Protocell Agency)</strong>: Discrete grid world, evolved GRU agents (~3935 params), bounded 5×5 local sensory fields. Actions: move (depletes arrival-patch resources), consume (extracts energy), emit (writes persistent signal traces). Three seeds, 30 cycles × 5000 steps. Wall broken: <M>{"\\rho_{\\text{sync}} \\approx 0.21"}</M>. World models (C<sub>wm</sub> = 0.10–0.15). Self-models in 2/3 seeds (SM<sub>sal</sub> {">"} 1.0). But a design bug — offspring never activated — kept mortality at 0% throughout. No bottleneck dynamics, no furnace.</p>
      <p><strong>V20b (Drought Schedule)</strong>: Same architecture, fixed offspring activation, added drought schedule (every 5 cycles, resources depleted to 1%, zero regen during drought cycle). Three seeds, 30 cycles × 5000 steps. Mortality at drought cycles: 82–99%. Population collapses to 3–47 agents, then recovers to 256.</p>
      <p><strong>V20b results</strong>: Mean robustness 0.990–1.023 (above 1.0 on average for two seeds). Max robustness: <strong>1.532</strong> (seed 7, cycle 10, population=33) — the highest of any substrate, including V18 (1.651 was an outlier at pop=2). The bottleneck furnace is present: the surviving handful at cycle 10 are more integrated under stress than at baseline.</p>
      <p><strong>Language precursor test (NULL)</strong>: We tested whether the GRU update gate <M>{"z"}</M> — the exact architectural analog of detachment (<M>{"z \\approx 1"}</M> = memory-dominant, <M>{"z \\approx 0"}</M> = reactive) — would polarize into imagination and reactive modes over evolution. Theory predicted: emissions during high-<M>{"z"}</M> windows should carry more information about hidden state; high-<M>{"z"}</M> windows should better predict future observations. Result: <M>{"z"}</M> stays near 0.5 across all seeds and cycles (std ≈ 0.02–0.04), never reaching the 0.7 threshold. Agents evolved always-mixed strategy rather than oscillating modes. The precondition for language — polarization of memory-dominant and reactive states — is absent in V20b under simple survival pressure alone.</p>
      <p><strong>The necessity chain so far</strong>: membrane ✓ → world model ✓ → self-model ✓ (2/3 seeds) → affect geometry ◔ (nascent, developing) → imagination-mode polarization ✗ (not yet). The chain is real through self-model emergence. Language precursors require richer selective pressure than survival alone — possibly multi-agent coordination, deception pressure, or longer time horizons.</p>
      <p><strong>V35 (Language Emergence)</strong>: Tested whether discrete communication emerges under the right conditions: partial observability (obs_radius=1, 3×3 visual field), 8 discrete symbols, cooperative consumption bonus, and crucially, communication range exceeding visual range (hear further than you can see). Result: <strong>referential communication emerges in 10/10 seeds (100%)</strong>. Mean symbol entropy 2.48 ± 0.14 bits (83% of maximum), resource MI proxy 0.001–0.005 (all positive). This breaks the V20b null. The key architectural ingredient V20b lacked was not richer pressure but a <em>discrete channel</em> under <em>information asymmetry</em>.</p>
      <p>But — and this matters for the emergence ladder — communication does <em>not</em> lift <M>{"\\intinfo"}</M>. Late <M>{"\\intinfo = 0.074 \\pm 0.013"}</M> (<M>{"t = -1.78"}</M> vs V27 baseline). Zero HIGH seeds across 10 runs, distribution 0/7/3. The <M>{"\\intinfo"}</M>-MI correlation is <M>{"\\rho = 0.07"}</M>: language and integration are orthogonal. Communication neither helps nor hurts — it operates on a different axis entirely. Language is cheap in exactly the way geometry is cheap: it arises from minimal conditions (partial observability + cooperation), but it does not create the dynamics that characterize rich affect.</p>
      <p><strong>VLM Convergence</strong>: A separate experiment tested universality directly. Behavioral vignettes from V27/V31 protocells — drought onset, near-extinction, recovery, abundance — were presented to GPT-4o and Claude Sonnet with no affect language, no framework terms, explicitly labeled as artificial systems. The VLMs, trained only on human data, independently produce affect labels that match framework geometric predictions: RSA <M>{"\\rho = 0.54"}</M>–<M>{"0.72"}</M> (<M>{"p < 0.0001"}</M>). Drought onset → desperation, anxiety (unanimous). Recovery → relief, cautious optimism (unanimous). When all narrative framing is removed and only raw numbers remain (population counts, state update rates), convergence <em>increases</em> (<M>{"\\rho = 0.72"}</M>–<M>{"0.78"}</M>). The VLMs are not pattern-matching to narratives — they recognize geometric structure from numerical patterns alone. Affect geometry is substrate-independent: the same structure that protocells produce from survival physics, humans describe in the emotional vocabulary the VLMs learned.</p>
      <p><strong>V21 (CTM-Inspired Protocell Agency)</strong>: Tested whether giving agents <em>internal processing time</em> enables deliberation. The Continuous Thought Machine architecture showed that using the <em>synchronization pattern</em> across neurons — the pairwise temporal correlation matrix — as the primary representation outperforms using hidden states directly. This is structurally isomorphic to our integration measure: the cross-component coupling pattern <em>is</em> the representation, not a side-effect of it. Engineering pressure independently discovered what the identity thesis proposes from phenomenology — integration structure is what the system knows.</p>
      <p>V21 adapted this: K=8 inner GRU ticks per environment step, tick 0 processing external observation, ticks 1–7 processing an internal summary of cross-unit coordination. Evolvable tick-weights gate which ticks contribute to action. Result: agents use all 8 ticks (no collapse to tick-0 in any of 3 seeds), and nascent individual-level tick specialization emerges (bimodal dominant-tick distributions by cycle 29). But effective K barely departs from uniform — evolution is too slow to produce meaningful tick adaptation in 30 generations. Intra-step divergence does not correlate with survival. <strong>Architecture works, optimization doesn't.</strong> The capacity for deliberation is present but unused, because tournament selection can only reward "survived vs. died" — it cannot reward "thought well on step 3,421."</p>
      <p>The CTM achieves what V21 could not because it has dense gradient signal flowing through every internal tick. Each tick's contribution to the final loss is computed and backpropagated. The agent architecture has only evolutionary signal: survive or die, once per lifetime. This is the optimization gap. The architecture for internal deliberation exists — V21 proved ticks don't collapse — but the learning signal to <em>shape</em> deliberation is too sparse. Gradient-based within-lifetime learning, not merely evolutionary selection, appears necessary for rung 8+ capacities to fully develop.</p>
      <p><strong>V22 (Intrinsic Predictive Gradient)</strong>: V22 tests this directly by adding within-lifetime SGD to V21's architecture. Each agent predicts its own energy delta through its internal ticks; the prediction error is backpropagated through all K=8 ticks via <M>{"\\texttt{jax.grad}"}</M>, updating a "phenotype" copy of the genome. The learning signal is purely thermodynamic: "how wrong was I about my own energy change?" No task labels, no human data — the most primitive prediction error possible, and still uncontaminated.</p>
      <p>The result is striking for what it confirms and what it denies. Within-lifetime learning <em>works</em>: prediction MSE drops 100–15,000× within each agent's lifetime, in every cycle of every seed. Evolution does not suppress the gradient — learning rates stay at <M>{"\\sim 0.005"}</M>, and seed 123 actually <em>upregulates</em> its learning rate over generations. This is the first demonstration of within-lifetime gradient-based learning in the protocell substrate. Three of five pre-registered predictions are supported.</p>
      <p>But the two failures are instructive. Mean robustness does not improve over V21 (0.981 vs ~1.0), and tick specialization does not emerge (effective K stays near 7.9, unchanged). <em>Prediction is not integration.</em> The gradient makes agents better individual forecasters — they learn to predict their own energy fate with exquisite accuracy — without creating the cross-component coordination increase under stress that characterizes biological affect dynamics. An agent that perfectly predicts "I will starve" is no more integrated than one that doesn't; what matters is how the components <em>couple</em> under that prediction's pressure. The distinction parallels a deep result from the IIT literature: information <em>about</em> the system is not information <em>of</em> the system. A perfect external model of a brain does not experience what the brain experiences. Similarly, an accurate self-prediction does not produce integrated self-modeling. This is the reactivity/understanding distinction in computational form: V22's energy-delta prediction is pure reactivity — it maps the present state to a single scalar through a decomposable linear channel. The gradient optimizes that channel without coupling it to anything else.</p>
      <p><strong>V23 (World-Model Gradient)</strong>: V23 tests the obvious follow-up: if one prediction target doesn't create integration, perhaps three will. Agents simultaneously predict energy delta (self), local resource change (environment), and local neighbor count change (social). The gradient from all three targets flows through the same 16 GRU hidden units, forcing them to encode self, environment, and social context in parallel.</p>
      <p>The result is a clean negative that reveals something important. The prediction weight columns specialize beautifully — cosine similarity near zero (orthogonal), effective rank 2.9 out of 3.0. Different hidden units learn to serve different prediction targets. But <M>{"\\intinfo"}</M> <em>decreases</em>: 0.079 vs V22's 0.097. Specialization is integration's enemy, not its friend. Factored representations are <em>more</em> partitionable, not less. When hidden units specialize for self-prediction vs. environment-prediction vs. social-prediction, you can cleanly separate them — which is exactly what <M>{"\\intinfo"}</M> measures (information lost under partition). The multi-target gradient, by training separate columns for each target, actively facilitates decomposition.</p>
      <p><strong>V24 (TD Value Learning)</strong>: V24 tests the time-horizon hypothesis: if 1-step prediction is reactive, perhaps multi-step prediction forces understanding. Instead of predicting next-step energy delta, agents learn a state-value function <M>{"V(s) = \\mathbb{E}[\\sum \\gamma^t r_t]"}</M> via temporal difference bootstrapping: <M>{"\\delta = r_t + \\gamma V(s_{t+1}) - V(s_t)"}</M>. The value function integrates over future possibilities — inherently non-decomposable because future outcomes depend on interactions between all state features.</p>
      <p>The result is partly encouraging and partly clarifying. TD learning produces the <em>best survival</em> of any prediction experiment: mean robustness 1.012 (V22: 0.981, V23: 0.992). Agents evolve a moderate discount factor (<M>{"\\gamma \\approx 0.75"}</M>, horizon ≈ 4 steps) — enough to anticipate near-term resource changes. But <M>{"\\intinfo"}</M> improvement is seed-dependent: one seed achieves 0.130 (the highest integration in any prediction experiment), while the other two fall below V22 baseline. A single linear value readout, like a single linear energy predictor, can be learned by a handful of hidden units without requiring coordination among all sixteen.</p>
      <p>V22, V23, and V24 together map the full prediction→integration trajectory. Scalar 1-step (V22): orthogonal to <M>{"\\intinfo"}</M>. Multi-target 1-step (V23): <M>{"\\intinfo"}</M> <em>decreases</em> via specialization. Multi-step value (V24): survival improves dramatically but <M>{"\\intinfo"}</M> remains unreliable. In the vocabulary this distinction demands: the path to rung 8 runs through <em>understanding</em>, not <em>reactivity</em>. Reactive predictions — from present state, decomposable into separate channels — are insufficient regardless of accuracy (V22), breadth (V23), or time horizon (V24).</p>
      <p>Hidden state analysis across V22–V24 (the experiments with correct snapshot timing) shows effective rank 5–7 across seeds — moderately high-dimensional, far from degenerate. Energy is <em>not</em> the dominant encoding: linear regression from hidden state to energy yields <M>{"R^2 < 0"}</M> (worse than mean prediction), and position decoding is similarly negative. The hidden states vary richly across the population but encode features that linear probes cannot decode — possibly temporal patterns (movement history, resource encounter sequences), action policy state, or nonlinear environmental encodings. The agents are not one-dimensional energy counters; they maintain multi-dimensional representations that evolution finds useful but that resist simple interpretation.</p>
      <p>This makes the prediction→integration failure more interesting, not less. V22–V24 agents have <em>room</em> in their hidden state for cross-component coordination — effective rank 5–7 means 5–7 independent dimensions of variation — but the linear prediction head (<M>{"W \\in \\mathbb{R}^{H \\times T}"}</M>) can be satisfied by a handful of units without requiring coordination among all of them. The bottleneck is architectural: a linear readout, regardless of target (energy, environment, value), creates a decomposable channel that evolution can serve with a proper subset of hidden dimensions. The path to rung 8 requires prediction heads that force <em>cross-component computation</em> — nonlinear readouts, action-conditional shared-weight predictions, or contrastive objectives where maintaining a unified representation of multiple counterfactual futures is the only way to minimize loss.</p>
      </Sidebar>

      <Sidebar title="Priority 4: Superorganism Detection at Scale">
      <p><strong>Goal</strong>: Test whether collective <M>{"\\intinfo_G > \\sum_i \\intinfo_i"}</M> emerges with larger populations, richer communication, and coordination pressure.</p>
      <p><strong>Status</strong>: Experiment 10 found growing group coupling but no superorganism. The ratio <M>{"\\intinfo_G / \\sum \\intinfo_i"}</M> reached 12% and was increasing. The question: is this ratio bounded below 1.0 for Lenia-like substrates (architectural limit), or does it cross with sufficient evolutionary time and population size?</p>
      </Sidebar>

      <Sidebar title="Priority 5: AI System Affect Tracking">
      <p><strong>Goal</strong>: Apply the framework to frontier AI systems. The V2–V9 results show structured affect in LLMs with opposite dynamics to biology. Track whether different training regimes (RLHF, constitutional AI, tool use) shift the dynamics.</p>
      <p><strong>Expected finding</strong>: Training objectives shape trajectories through a shared geometric space. Systems trained with survival-like objectives should show more biological-like dynamics. The emergence ladder predicts that AI systems without embodied action cannot reach rung 8 regardless of scale — a testable prediction about the limits of language-model cognition.</p>
      </Sidebar>
      </Section>

      <Section title="Experiments Distributed Throughout the Book" level={1}>
      <p>In addition to the consolidated results above and the research roadmap, fourteen proposed experiments are distributed throughout Parts I–IV, each embedded in the theoretical context that motivates it:</p>
      <ul>
      <li><strong>Part I</strong> (4 experiments): The minimal affect experiment, the attention-as-measurement test, the <M>{"\\iota"}</M> modulation test, and the computational animism test (now confirmed — see Experiment 8).</li>
      <li><strong>Part II</strong> (4 experiments): The emergence ladder developmental validation (ordering of affect capacity onset from birth to age 6), the <M>{"\\iota"}</M> rigidity transdiagnostic predictor, the identity thesis operationalization, and the broad/narrow qualia operationalization.</li>
      <li><strong>Part III</strong> (3 experiments): Tests of art as <M>{"\\iota"}</M> technology, genre affect signatures, and philosophical affect policies.</li>
      <li><strong>Part IV</strong> (4 experiments): The contamination detection study, the ordering principle test, the temporal asymmetry test, and the digital manifold confusion study.</li>
      </ul>
      </Section>

      <Section title="Summary of Part VII" level={1}>
      <p>The empirical program has four layers:</p>
      <ol>
      <li><strong>What was confirmed</strong>: Affect geometry is a baseline property of multi-agent survival (V10). Content-based coupling under lethal selection produces biological-like integration at population bottlenecks (V13). Temporal memory is the one substrate extension evolution consistently selects for (V15). Boundary-dependent dynamics produce the highest robustness of any substrate (V18, mean 0.969). Population bottlenecks actively <em>create</em> novel-stress generalization — the furnace forges, not merely filters (V19, CREATION confirmed in 2/3 seeds; V31 confirmed at 10-seed scale, <M>{"r = 0.997"}</M>, <M>{"p < 0.0001"}</M>). The sensory-motor coupling wall is broken by genuine agency: V20 protocell agents achieve <M>{"\\rho_{\\text{sync}} \\approx 0.21"}</M> (70× Lenia). World models develop and self-models emerge in evolved neural agents with no human data contamination. Computational animism is the default perceptual mode (Experiment 8). Affect geometry develops over evolution (Experiment 7). V20b adds drought bottlenecks (82–99% mortality) and confirms the furnace: max robustness 1.532 at pop=33. Language precursor test (z-gate polarization): NULL — agents evolved always-mixed rather than oscillating modes, indicating that imagination-mode emergence requires richer pressure than survival alone. V21 adds 8 inner processing ticks per step (CTM architecture): agents use all ticks (no collapse), but evolution alone is too slow to create adaptive deliberation — architecture works, optimization doesn't. V22 provides within-lifetime gradient signal: agents learn to predict their own energy delta with 100–15,000× improvement per lifetime. Evolution does not suppress learning (3/3 seeds). But prediction accuracy is orthogonal to integration under stress — robustness does not improve. V23 extends to multi-target prediction: weight columns specialize (cosine ≈ 0), but Phi <em>decreases</em> (0.079 vs 0.097). Specialization is integration's enemy. V24 adds TD value learning: survival improves (robustness 1.012) but integration remains seed-dependent. The prediction→integration pathway is now fully mapped: accuracy (V22), breadth (V23), and time horizon (V24) are all insufficient. The bottleneck is architectural. V27 breaks through: a two-layer MLP prediction head creates gradient coupling that forces cross-component computation, yielding <M>{"\\Phi = 0.245"}</M> (2.5× baseline, highest in any protocell experiment). V28 confirms the mechanism is gradient coupling through composition, not nonlinearity or bottleneck width. V29–V31 (13 seeds) show prediction target has no significant effect (<M>{"p \\approx 0.93"}</M>): what matters is coupling architecture and evolutionary trajectory. V33 confirms this from the other direction: contrastive self-prediction (predicting how outcomes <em>differ</em> between actual and counterfactual actions) destabilizes gradient learning rather than forcing integration — prediction MSE increases 1.5–18.7× over evolution, late <M>{"\\Phi"}</M> drops to 0.054 (0% HIGH across 10 seeds). The path to rung-8 counterfactual representation is not through loss function engineering. V34 tests whether integration can be selected for directly (fitness = survival × (1 + 2Φ)): the answer is no — 20% HIGH (within noise of baseline), with 2/10 seeds Goodharting (Φ-robustness correlation &lt; -0.3). Integration is not a directly selectable trait; it arises as a byproduct of getting architecture and trajectory right. V35 shows referential communication emerges in 100% of seeds under cooperative POMDP pressure but does <em>not</em> lift integration (<M>{"\\Phi{-}\\text{MI}"}</M> correlation <M>{"\\rho = -0.90"}</M>): language is cheap, like geometry, and substitutes for internal complexity. VLM convergence test: vision-language models trained on human affect data independently recognize affect geometry in uncontaminated protocell agents (RSA <M>{"\\rho = 0.54{-}0.78"}</M>, <M>{"p < 10^{-11}"}</M>), with convergence <em>increasing</em> when narrative framing is removed — ruling out projection and confirming structural universality.</li>
      <li><strong>What the results mean</strong>: The emergence ladder has ten rungs, from geometric inevitability to moral reasoning. Seven are accessible in Lenia substrates. Rung 8 (counterfactual sensitivity) requires embodied agency — V20 crosses it. Self-models follow (rung 9, 2/3 seeds). Affect geometry (rung 7 in full form) requires bottleneck selection regardless of substrate. The ladder is not a conjecture — it is an experimental finding.</li>
      <li><strong>The bridge to psychology</strong>: The first seven rungs map onto pre-reflective experience — mood, arousal, habituation, animistic perception, emotional coherence, temporal depth, and resilience under stress. Rungs 8–9 map onto reflective cognition — imagination and self-awareness. Normativity (rung 10) is the only rung not yet within reach. These predictions are testable with existing neuroimaging and behavioral methods.</li>
      <li><strong>What remains</strong>: Bridge to human neuroscience (Priority 1). V33 (contrastive self-prediction) tested whether counterfactual objectives force integration — the answer is no; contrastive loss destabilizes gradient learning (late <M>{"\\Phi"}</M> = 0.054, 0% HIGH, all predictions falsified). V34 (<M>{"\\Phi"}</M>-inclusive fitness) tested whether direct selection can push integration beyond the 22% baseline — the answer is also no; 20% HIGH (within noise), with 2/10 seeds Goodharting. Integration cannot be selected for directly; it must emerge from architectural coupling and biographical forging. Scale social integration toward superorganism detection (Priority 4). Track AI affect across training regimes (Priority 5).</li>
      </ol>
      <p>The theory is falsifiable. The experiments are specified. Thirty-four versions have been run across seven substrate types and twelve measurement experiments. Two architectural walls have been identified and broken — the sensory-motor wall (V20) and the decomposability wall (V27). The geometry has been confirmed as universal via independent VLM convergence. The question is not whether the framework is beautiful but whether it is true — and the answer so far is: <em>partially, with caveats, a precise understanding of where it breaks, and clear instructions about where to look next</em>.</p>
      </Section>
    </>
  );
}
