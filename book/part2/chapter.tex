\begin{logos}
This entire high-dimensional trajectory through a space that has real geometric structure, real basins and ridges and gradients, is not something separate from the physical process, not an emergent epiphenomenon floating mysteriously above the neural dynamics, but rather is identical to the intrinsic cause-effect structure itself, the view from inside of what these causal relations feel like when you are those causal relations, when there is no homunculus sitting somewhere else observing the process but only the process itself, recursively modeling its own modeling, predicting its own predictions.
\end{logos}

%==============================================================================
\section{The Hard Problem and Its Dissolution}
%==============================================================================

\begin{connection}
This section engages with the central debates in philosophy of mind:
\begin{itemize}
\item \textbf{Chalmers' Hard Problem} (1995): The explanatory gap between physical processes and phenomenal experience. I think this gap results from a category error, not a genuine ontological divide.
\item \textbf{Nagel's ``What Is It Like''} (1974): The subjective character of experience. I'll formalize this as intrinsic cause-effect structure---what the system is \emph{for itself}.
\item \textbf{Jackson's Knowledge Argument} (1982): Mary the colorblind scientist. My reinterpretation: Mary gains \emph{access to a new scale of description}, not new facts about the same scale.
\item \textbf{Eliminativism} (Churchland, 1981; Dennett, 1991): Consciousness as illusion. I reject this---the illusion would itself be experiential, hence self-refuting.
\item \textbf{Panpsychism} (Chalmers, 2015; Goff, 2017): Experience as fundamental. I accept a version: cause-effect structure at any scale that takes/makes differences has a form of ``being like.''
\end{itemize}
\end{connection}

\subsection{The Standard Formulation}

The ``hard problem'' of consciousness asks: given a complete physical description of a system, why is there something it is like to be that system? How does experience arise from non-experience?

Formally, let $\mathcal{D}^{\text{phys}}$ be a complete physical description of a system---its particles, fields, dynamics, everything describable in third-person terms. The hard problem asserts:
\begin{equation}
\mathcal{D}^{\text{phys}} \not\Rightarrow \mathcal{D}^{\text{phen}}
\end{equation}
where $\mathcal{D}^{\text{phen}}$ is a description of the system's phenomenal properties (what it's like to be it). The claim is that no amount of physical information logically entails phenomenal information.

This formulation rests on a crucial assumption:

\begin{axiom}[Privileged Base Layer---REJECTED]
\label{ax:base}
Physics constitutes a privileged ontological base layer. All other descriptions (chemical, biological, psychological, phenomenal) are ``higher-level'' and must reduce to or supervene on the physical description. What is ``really real'' is what physics describes.
\end{axiom}

I reject this axiom.

\subsection{Ontological Democracy}

Consider the standard reductionist hierarchy:
\begin{center}
\begin{tikzpicture}[
    node distance=0.45cm,
    box/.style={rectangle, draw, rounded corners, minimum width=4cm, minimum height=0.45cm, align=center, font=\small}
]
% Top-down gradient: violet (mind) to red (physics) to gray (unknown)
\node[box, fill=violet!15, draw=violet!60!black] (phenom) {Phenomenal};
\node[box, fill=blue!15, draw=blue!60!black, below=of phenom] (psych) {Psychological};
\node[box, fill=cyan!15, draw=cyan!60!black, below=of psych] (bio) {Biological};
\node[box, fill=green!15, draw=green!60!black, below=of bio] (chem) {Chemical};
\node[box, fill=yellow!15, draw=yellow!60!black, below=of chem] (atom) {Atomic};
\node[box, fill=orange!15, draw=orange!60!black, below=of atom] (subatom) {Subatomic};
\node[box, fill=red!15, draw=red!60!black, below=of subatom] (qft) {Quantum Fields};
\node[box, fill=gray!20, draw=gray!60!black, below=of qft] (base) {\textbf{???}};

\draw[-{Stealth}, thick, gray] (phenom) -- (psych) node[midway, right, font=\footnotesize, text=gray!70!black] {reduces?};
\draw[-{Stealth}, thick, gray] (psych) -- (bio);
\draw[-{Stealth}, thick, gray] (bio) -- (chem);
\draw[-{Stealth}, thick, gray] (chem) -- (atom);
\draw[-{Stealth}, thick, gray] (atom) -- (subatom);
\draw[-{Stealth}, thick, gray] (subatom) -- (qft);
\draw[-{Stealth}, thick, gray, dashed] (qft) -- (base);

% Brace on left indicating "equally real"
\draw[decorate, decoration={brace, amplitude=8pt, mirror}, thick, gray!60]
    ([xshift=-0.3cm]phenom.north west) -- ([xshift=-0.3cm]qft.south west)
    node[midway, left=0.4cm, font=\scriptsize, text=gray, align=center, rotate=90] {equally real?};
\end{tikzpicture}
\end{center}

At each level, one might claim the higher level ``reduces to'' the lower. But the regression terminates in uncertainty:
\begin{itemize}
\item Wave functions are descriptions of probability distributions
\item Probability amplitudes describe which interactions are more or less likely
\item What ``actually happens'' when a measurement occurs is deeply contested
\item Below quantum fields, we have no clear ontology at all
\end{itemize}

The supposed ``base layer'' turns out to be:
\begin{enumerate}
\item Probabilistic, not deterministic
\item Descriptive, not fundamental (wave functions are representations)
\item Incomplete (we don't know what underlies field interactions)
\item Not clearly more ``real'' than any other scale of description
\end{enumerate}

The alternative I propose is \textbf{ontological democracy}: every scale of structural organization with its own causal closure is \emph{equally real} at that scale. No layer is privileged as ``the'' fundamental reality. Each layer (a) has its own causal structure, (b) has its own dynamics and laws, (c) exerts influence on adjacent layers (both ``up'' and ``down''), (d) is incomplete as a description of the whole, and (e) is sufficient for phenomena at its scale.

Once this is granted, the demand that phenomenal properties ``reduce to'' physical properties is ill-posed. Chemistry doesn't reduce to physics in a way that eliminates chemical causation---chemical causation is real at the chemical scale. Similarly, phenomenal properties don't need to reduce to physical properties---they are real at the phenomenal scale.

\subsection{Existence as Causal Participation}

We need a criterion for existence that applies uniformly across scales---here "we" means anyone trying to think clearly about this.

The criterion I adopt is this: an entity $X$ \emph{exists} at scale $\sigma$ if and only if
\begin{equation}
\exists Y: \MI(X; Y | \text{background}_\sigma) > 0
\end{equation}
That is, $X$ takes and makes differences at scale $\sigma$. It participates in causal relations at that scale.

\begin{example}
\begin{itemize}
\item An electron exists at the quantum scale: it takes differences (responds to fields) and makes differences (affects measurements).
\item A cell exists at the biological scale: it takes differences (nutrients, signals) and makes differences (metabolism, division, death).
\item An experience exists at the phenomenal scale: it takes differences (sensory input, memory) and makes differences (attention, behavior, learning).
\end{itemize}
\end{example}

This is closely aligned with IIT's foundational axiom: to exist is to have cause-effect power. But we extend it: cause-effect power at any scale constitutes existence at that scale, with no scale privileged.

\subsection{The Dissolution}

The hard problem asked: how do you get experience from non-experience? The answer is: \textit{you don't need to}.

Just as chemistry doesn't emerge from non-chemistry---you have chemistry when you have the right causal organization at the chemical scale---experience doesn't emerge from non-experience. You have experience when you have the right causal organization at the experiential scale.

The question ``why is there something it's like to be this system?'' is exactly as deep as ``why does chemistry exist?'' or ``why are there quantum fields?'' I don't know why there's anything at all (idk if anybody does). But given that there's anything, the emergence of self-modeling systems with integrated cause-effect structure is not mysterious---it's typical.

\begin{keyresult}
The hard problem dissolves not because we answered it, but because we showed it was asking for a privilege (reduction to physics) that physics itself doesn't have.
\end{keyresult}

\begin{sidebar}[title=The Hard Problem as Perceptual Artifact]
The hard problem has a further wrinkle, which will become clearer after we introduce the inhibition coefficient $\iota$ later in this part. The question ``why is there something it's like to be this system?'' is asked from a perceptual configuration that has already factorized experience into ``physical process'' and ``felt quality'' so thoroughly that reconnecting them seems impossible. At lower $\iota$---in the participatory mode where affect and perception are not yet factored apart---the question does not arise with the same force. Not because it has been answered, but because the factorization that generates it has not been performed. The explanatory gap may be partly a perception-mode artifact: a consequence of the mechanistic mode's success at separating things that, in experience, were never separate.
\end{sidebar}

%==============================================================================
\section{The Identity Thesis}
%==============================================================================

\begin{connection}
The identity thesis is a formalization of \textbf{Integrated Information Theory (IIT)} developed by Giulio Tononi and collaborators (2004--present):
\begin{itemize}
\item \textbf{IIT 1.0} (Tononi, 2004): Introduced $\Phi$ as a measure of integrated information
\item \textbf{IIT 2.0} (Balduzzi \& Tononi, 2008): Added the concept of ``qualia space''
\item \textbf{IIT 3.0} (Oizumi, Albantakis \& Tononi, 2014): Full axiom/postulate structure; introduced cause-effect structure
\item \textbf{IIT 4.0} (Albantakis et al., 2023): Refined integration measures, introduced intrinsic difference
\end{itemize}

Key IIT axioms that we adopt:
\begin{enumerate}
\item \textbf{Intrinsicality}: Experience exists for itself, not for an external observer
\item \textbf{Information}: Experience is specific---this experience and no other
\item \textbf{Integration}: Experience is unified and irreducible
\item \textbf{Exclusion}: Experience has definite boundaries
\item \textbf{Composition}: Experience is structured
\end{enumerate}

My contribution here is connecting IIT's structural characterization to (1) the thermodynamic ladder, (2) the viability manifold, and (3) operational measures for artificial systems.
\end{connection}

\subsection{Statement of the Thesis}

The thesis is an identity claim: phenomenal experience \textit{is} intrinsic cause-effect structure. Not caused by it, not correlated with it, but identical to it. The phenomenal properties of an experience (what it's like) just are the structural properties of the system's internal causal relations, described from the intrinsic perspective.

To make this precise, we need two notions. The \textbf{cause-effect structure} $\cestructure(\mathcal{S}, \state)$ of a system $\mathcal{S}$ in state $\state$ is the complete specification of: (a) all distinctions $\{\distinction_i\}$---subsets of the system's elements in their current states; (b) the cause repertoire of each distinction, $p(\text{past} | \distinction_i)$; (c) the effect repertoire, $p(\text{future} | \distinction_i)$; (d) all relations $\{\relation_{ij}\}$---overlaps and connections between distinctions' causes and effects; and (e) the irreducibility of each distinction and relation. The \textbf{intrinsic perspective} is the description of this structure without reference to any external observer, coordinate system, or comparison class---the structure as it exists for the system itself.

\begin{axiom}[IIT Identity]
\begin{equation}
\phenom(\mathcal{S}, \state) \equiv \cestructure^{\text{intrinsic}}(\mathcal{S}, \state)
\end{equation}
The phenomenal structure $\phenom$ is identical to the intrinsic cause-effect structure $\cestructure$.
\end{axiom}

This is not a correlation claim or a supervenience claim. It is an identity claim, analogous to:
\begin{equation}
\text{Water} \equiv \text{H}_2\text{O}
\end{equation}

\subsection{Implications for the Zombie Argument}

The philosophical zombie is supposed to be conceivable: a system physically/functionally identical to a conscious being but lacking experience. If conceivable, experience isn't necessitated by physical structure.

Under the identity thesis, philosophical zombies are not coherently conceivable. A system with the relevant cause-effect structure \textit{is} an experience; there is no further fact about whether it ``really'' has phenomenal properties.

\begin{proof}
By the identity thesis, $\phenom \equiv \cestructure^{\text{intrinsic}}$. To conceive a zombie is to conceive a system with $\cestructure^{\text{intrinsic}}$ but without $\phenom$. But since these are identical, this is like conceiving of water without H$_2$O---not genuinely conceivable once the identity is understood.
\end{proof}

\subsection{The Structure of Experience}

If experience is cause-effect structure, then the \textit{kind} of experience is determined by the \textit{shape} of that structure. Different phenomenal properties correspond to different structural features.

IIT proposes that the essential properties of any experience are:

\begin{enumerate}
\item \textbf{Intrinsicality}: The experience exists for the system itself, not relative to an external observer.
\item \textbf{Information}: The experience is specific---this experience, not any other possible one.
\item \textbf{Integration}: The experience is unified---it cannot be decomposed into independent sub-experiences.
\item \textbf{Exclusion}: The experience has definite boundaries---there is a fact about what is and isn't part of it.
\item \textbf{Composition}: The experience is structured---composed of distinctions and relations among them.
\end{enumerate}

These are translated into physical/structural postulates:
\begin{itemize}
\item Intrinsicality $\to$ Cause-effect power within the system
\item Information $\to$ Specific cause-effect repertoires
\item Integration $\to$ Irreducibility to partitioned components
\item Exclusion $\to$ Maximality of the integrated complex
\item Composition $\to$ The full structure of distinctions and relations
\end{itemize}

\begin{sidebar}[title=Engaging with IIT Criticisms]
The identity thesis inherits IIT's strengths and its controversies. Intellectual honesty requires engaging with the most serious objections.

\textbf{The expander graph problem} (Aaronson, 2014): Simple systems like grid networks may have very high $\intinfo$ under IIT's formalism despite seeming clearly non-conscious. If $\intinfo$ tracks consciousness, even grid wiring diagrams are richly experiential. \emph{Response}: This objection targets exact $\intinfo$ as defined by IIT 3.0's formalism. The framework here works with proxies---partition prediction loss, spectral effective rank, coupling-weighted covariance---that are calibrated against systems with known behavioral and structural properties (biological organisms, trained agents, evolved CA patterns). Whether exact $\intinfo$ maps onto consciousness for arbitrary mathematical structures is a question about the formalism, not about the structural principle. The claim is not ``any system with high $\intinfo$ is conscious'' but ``experience is integrated cause-effect structure at the appropriate scale,'' where ``appropriate'' is constrained by the full structural profile, not a single number.

\textbf{Computational intractability}: Exact $\intinfo$ is NP-hard to compute for systems beyond trivial size. \emph{Response}: Acknowledged. The V11 experiments (Part I) use spectral proxies validated by convergence with exact measures on small systems. All empirical claims rest on proxies, not exact $\intinfo$. This is analogous to using Boltzmann entropy rather than Gibbs entropy for practical calculations---the conceptual definition and the computational tool can diverge without invalidating either.

\textbf{Over-attribution}: If any system with $\intinfo > 0$ is conscious, thermostats are conscious. \emph{Response}: The gradient of distinction (Part I, Section 1) makes this explicit. Yes, a thermostat has minimal cause-effect structure. Whether that constitutes minimal experience or no experience is an empirical question the framework does not prematurely answer. The important claim is that there is a \emph{continuum}, not a binary threshold. The framework's six affect dimensions are measurably present only in systems with substantial integration, self-modeling, and viability maintenance---not in thermostats.

\textbf{The real vulnerability}: The identity thesis, like any metaphysical identity claim, cannot be empirically verified in the standard sense. You cannot compare experience ``from the outside'' with cause-effect structure ``from the inside'' because there is no vantage point from which both are simultaneously accessible. What can be tested: whether the structural predictions (affect motifs, dimensional clustering, Î¹ dynamics) track human phenomenal reports and behavioral measures. If they do, the identity thesis gains inductive support. If they do not, the structural framework fails regardless of the metaphysics.
\end{sidebar}

%==============================================================================
\section{The Geometry of Affect}
%==============================================================================

\begin{connection}
My geometric theory of affect builds on and extends established dimensional models:
\begin{itemize}
\item \textbf{Russell's Circumplex Model} (1980): Two-dimensional (valence $\times$ arousal) organization of affect. I extend this with additional structural dimensions (integration, effective rank, counterfactual weight, self-model salience) invoked as needed.
\item \textbf{Watson \& Tellegen's PANAS} (1988): Positive/Negative Affect Schedule. My valence dimension corresponds to their hedonic axis.
\item \textbf{Scherer's Component Process Model} (2009): Emotions as synchronized changes across subsystems. My integration measure $\intinfo$ captures this synchronization.
\item \textbf{Barrett's Constructed Emotion Theory} (2017): Emotions as constructed from core affect + conceptual knowledge. My framework specifies the \emph{structural} basis of the construction.
\item \textbf{Damasio's Somatic Marker Hypothesis} (1994): Body states guide decision-making. My valence definition (gradient on viability manifold) is the mathematical formalization.
\end{itemize}
\end{connection}

\begin{sidebar}[title=On Dimensionality]
I'm not claiming that six dimensions are necessary or sufficient for characterizing all affect. These are a \emph{useful} coordinate system, not \emph{the} coordinate system. Just as Cartesian coordinates serve some problems and polar coordinates serve others, these dimensions are tools for thought, not discoveries of essence. Different phenomena may require different subsets:
\begin{itemize}
\item Some affects are essentially \textbf{two-dimensional} (valence + arousal suffices for basic mood)
\item Others require \textbf{self-referential structure} (shame requires high $\mathcal{SM}$; flow requires low $\mathcal{SM}$)
\item Still others are defined by \textbf{temporal structure} (grief requires persistent counterfactual coupling to the lost object)
\item Some may require dimensions not in this list (anger requires ``other-model compression'')
\end{itemize}
The dimensions below form a \emph{toolkit}---structural features that may or may not matter for any given phenomenon. Empirical investigation may reveal that some proposed dimensions are redundant, or that additional dimensions are needed. I'll invoke only what is necessary.
\end{sidebar}

\subsection{Affects as Structural Motifs}

If different experiences correspond to different structures, then \textit{affects}---the qualitative character of emotional/valenced states---should correspond to particular structural motifs: characteristic patterns in the cause-effect geometry.

The \emph{affect space} $\mathcal{A}$ is a geometric space whose points correspond to possible qualitative states. Rather than fixing a universal dimensionality, we identify the structural features that define each affect---features without which that affect would not be that affect.

The following structural measures form a toolkit for characterizing affect. Not all are relevant to every phenomenon; I invoke each only when it does essential work:

\begin{description}
\item[Valence ($\valence$)] Gradient alignment on the viability manifold. Nearly universal---most affects have valence.
\item[Arousal ($\arousal$)] Rate of belief/state update. Distinguishes activated from quiescent states.
\item[Integration ($\intinfo$)] Irreducibility of cause-effect structure. Constitutive for unified vs.\ fragmented experience.
\item[Effective Rank ($\effrank$)] Distribution of active degrees of freedom. Constitutive when the contrast between expansive and collapsed experience matters.
\item[Counterfactual Weight ($\mathcal{CF}$)] Resources allocated to non-actual trajectories. Constitutive for affects defined by temporal orientation (anticipation, regret, planning).
\item[Self-Model Salience ($\mathcal{SM}$)] Degree of self-focus in processing. Constitutive for self-conscious emotions and their opposites (absorption, flow).
\end{description}

\subsection{Valence: Gradient Alignment}

Let $\viable$ be the system's viability manifold and let $\mathbf{x}_t$ be the current state. Let $\hat{\mathbf{x}}_{t+1:t+H}$ be the predicted trajectory under current policy. Then valence measures the alignment of that trajectory with the viability gradient:
\begin{equation}
\valence_t = -\frac{1}{H} \sum_{k=1}^{H} \gamma^k \nabla_{\mathbf{x}} d(\mathbf{x}, \partial\viable) \bigg|_{\hat{\mathbf{x}}_{t+k}} \cdot \frac{d\hat{\mathbf{x}}_{t+k}}{dt}
\end{equation}
where $d(\cdot, \partial\viable)$ is the distance to the viability boundary. Positive valence means the predicted trajectory moves into the viable interior; negative valence means it approaches the boundary.

In RL terms, this becomes the expected advantage of the current action---how much better (or worse) it is than the average action from this state:
\begin{equation}
\valence_t = \E_{\policy}\left[ A^{\policy}(\state_t, \action_t) \right] = \E_{\policy}\left[ Q^{\policy}(\state_t, \action_t) - V^{\policy}(\state_t) \right]
\end{equation}

Beyond valence itself, its rate of change carries structural information. The derivative of integrated information along the trajectory,
\begin{equation}
\dot{\valence}_t = \frac{d\intinfo}{dt}\bigg|_{\hat{\mathbf{x}}_{t:t+H}}
\end{equation}
tracks whether structure is expanding (positive $\dot{\valence}$) or contracting (negative).

\begin{phenomenal}
\textbf{Positive valence} corresponds to trajectories descending the free-energy landscape, expanding affordances, moving toward sustainable states. \\
\textbf{Negative valence} corresponds to trajectories ascending toward constraint violation, contracting possibilities.
\end{phenomenal}

\begin{sidebar}[title=Valence in Discrete Substrate]
In a cellular automaton or other discrete dynamical system, valence becomes exactly computable:
\begin{itemize}
\item $\viable$ = configurations where the pattern persists
\item $\partial\viable$ = configurations where the pattern dissolves
\item $d(\mathbf{x}, \partial\viable)$ = minimum Hamming distance to a non-viable state
\item Trajectory = sequence of configurations $\mathbf{x}_1, \mathbf{x}_2, \ldots$
\end{itemize}
Then:
\begin{equation}
\valence_t = d(\mathbf{x}_{t+1}, \partial\viable) - d(\mathbf{x}_t, \partial\viable)
\end{equation}
Positive when the pattern moves away from dissolution; negative when approaching it; zero when maintaining constant distance. For a glider cruising through empty space: $\valence \approx 0$. For a glider approaching collision: $\valence < 0$. For a pattern that just escaped a near-collision: $\valence > 0$.

This is not metaphor---it is the viability gradient formalized for discrete state spaces.
\end{sidebar}

\subsection{Arousal: Update Rate}

Arousal measures how rapidly the system is revising its world model. The natural formalization is the KL divergence between successive belief states:
\begin{equation}
\arousal_t = \KL\left( \belief_{t+1} \| \belief_t \right) = \sum_{\mathbf{x}} \belief_{t+1}(\mathbf{x}) \log \frac{\belief_{t+1}(\mathbf{x})}{\belief_t(\mathbf{x})}
\end{equation}

In latent-space models, this can be approximated more directly:
\begin{equation}
\arousal_t = \| \latent_{t+1} - \latent_t \|^2 \quad \text{or} \quad \MI(\obs_t; \latent_{t+1} | \latent_t, \action_t)
\end{equation}

\begin{phenomenal}
\textbf{High arousal}: Large belief updates, far from any attractor, system actively navigating. \\
\textbf{Low arousal}: Near a fixed point, low surprise, system at rest in a basin.
\end{phenomenal}

\subsection{Integration: Irreducibility}

As defined in Part I:
\begin{equation}
\intinfo(\state) = \min_{\text{partitions } P} D\left[ p(\state_{t+1} | \state_t) \| \prod_{p \in P} p(\state^p_{t+1} | \state^p_t) \right]
\end{equation}

Or using proxies:
\begin{equation}
\intinfo_{\text{proxy}} = \Delta_P = \mathcal{L}_{\text{pred}}[\text{partitioned}] - \mathcal{L}_{\text{pred}}[\text{full}]
\end{equation}

\begin{phenomenal}
\textbf{High integration}: The experience is unified; its parts cannot be separated without loss. \\
\textbf{Low integration}: The experience is fragmentary or modular.
\end{phenomenal}

\begin{sidebar}[title=Integration in Discrete Substrate]
In a cellular automaton, $\intinfo$ is directly computable for small patterns:
\begin{enumerate}
\item Define the pattern as cells $\{c_1, c_2, \ldots, c_n\}$
\item For each bipartition $P = (A, B)$:
\begin{itemize}
\item Compute $p(\state_{t+1} | \state_t)$ for the whole pattern
\item Compute $p_A(\state^A_{t+1} | \state^A_t) \times p_B(\state^B_{t+1} | \state^B_t)$ for independent parts
\item Measure divergence $D[p \| p_A \times p_B]$
\end{itemize}
\item $\intinfo = \min_P D$
\end{enumerate}
High $\intinfo$ means you cannot partition the pattern without losing predictive power. The parts must be considered together.

For a simple glider: $\intinfo$ is probably modest (only 5 cells). For a complex pattern with tightly coupled components: $\intinfo$ can be high. The key empirical question: does high $\intinfo$ correlate with survival, behavioral complexity, or adaptive response to perturbation?
\end{sidebar}

\subsection{Effective Rank: Concentration vs. Distribution}

The dimensionality of a system's active representation can be quantified through the effective rank of its state covariance $C$:
\begin{equation}
\effrank = \frac{(\tr C)^2}{\tr(C^2)} = \frac{\left(\sum_i \lambda_i\right)^2}{\sum_i \lambda_i^2}
\end{equation}
When $\effrank \approx 1$, all variance is concentrated in a single dimension---the system is maximally collapsed. When $\effrank \approx n$, variance distributes uniformly across all available dimensions---the system is maximally expanded.

\begin{phenomenal}
\textbf{High rank}: Many degrees of freedom active; distributed, expansive experience. \\
\textbf{Low rank}: Collapsed into narrow subspace; concentrated, focused, or trapped experience.
\end{phenomenal}

\begin{sidebar}[title=Effective Rank in Discrete Substrate]
For a pattern in a CA, record its trajectory $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T$ (configuration at each timestep). Each configuration is a point in $\{0,1\}^n$. Compute the covariance matrix $C$ of these binary vectors treated as $\R^n$ points.

For a glider: the trajectory lies on a low-dimensional manifold (position $\times$ position $\times$ phase $\approx 3$--$4$ effective dimensions out of $n$ cells). $\effrank$ is small.

For a complex evolving pattern: the trajectory may explore many independent dimensions. $\effrank$ is large.

The thesis predicts this maps to phenomenology:
\begin{itemize}
\item Joy: high $\effrank$ (expansive, many active possibilities)
\item Suffering: low $\effrank$ (collapsed, trapped in narrow manifold)
\end{itemize}
In discrete substrate, this is not metaphor but measurement.
\end{sidebar}

\subsection{Counterfactual Weight}

Where the previous dimensions captured the system's current state, counterfactual weight captures its temporal orientation---how much processing is devoted to possibilities rather than actualities. Let $\mathcal{R}$ be the set of imagined rollouts (counterfactual trajectories) and $\mathcal{P}$ be present-state processing. Then:
\begin{equation}
\mathcal{CF}_t = \frac{\text{Compute}_t(\mathcal{R})}{\text{Compute}_t(\mathcal{R}) + \text{Compute}_t(\mathcal{P})}
\end{equation}
The fraction of computational resources devoted to modeling non-actual possibilities.

In model-based RL:
\begin{equation}
\mathcal{CF}_t = \sum_{\tau \in \text{rollouts}} w(\tau) \cdot \entropy[\tau] \quad \text{where} \quad w(\tau) \propto |V(\tau)|
\end{equation}
Rollouts weighted by their value magnitude and diversity.

\begin{phenomenal}
\textbf{High counterfactual weight}: Mind is elsewhere---planning, worrying, fantasizing, anticipating. \\
\textbf{Low counterfactual weight}: Present-focused, reactive, in-the-moment.
\end{phenomenal}

\begin{sidebar}[title=Counterfactual Weight in Discrete Substrate]
For most CA patterns: $\mathcal{CF} = 0$. They follow their dynamics without simulation.

But Life contains universal computers---patterns that can simulate arbitrary computations, including Life itself. Imagine a pattern $\mathcal{B}$ containing:
\begin{itemize}
\item A simulator subregion that runs a model of possible futures
\item A controller that adjusts behavior based on simulator output
\end{itemize}
Then:
\begin{equation}
\mathcal{CF} = \frac{|\text{simulator cells}|}{|\mathcal{B}|}
\end{equation}
The fraction of the pattern devoted to counterfactual reasoning.

Such patterns are rare and complex---universal computation requires many cells. But they should outperform simple patterns: they can anticipate threats (fear structure) and identify opportunities (desire structure). The prediction: patterns with $\mathcal{CF} > 0$ survive longer in hostile environments.
\end{sidebar}

\subsection{Self-Model Salience}

The final dimension measures how prominently the self figures in the system's own processing. Self-model salience is the fraction of action entropy explained by the self-model component:
\begin{equation}
\mathcal{SM}_t = \MI(\latent^{\text{self}}_t; \action_t) / \entropy(\action_t)
\end{equation}

Alternatively:
\begin{equation}
\mathcal{SM}_t = \frac{\text{dim}(\latent^{\text{self}})}{\text{dim}(\latent^{\text{total}})} \cdot \text{activity}(\latent^{\text{self}}_t)
\end{equation}

\begin{phenomenal}
\textbf{High self-salience}: Self-focused, self-conscious, self as primary object of attention. \\
\textbf{Low self-salience}: Self-forgotten, absorbed in environment or task.
\end{phenomenal}

\begin{sidebar}[title=Self-Model Salience in Discrete Substrate]
In a CA, a pattern's ``behavior'' is its evolution. Let $\latent^{\text{self}}$ denote cells that track the pattern's own state (the self-model region). Then:
\begin{equation}
\mathcal{SM} = \frac{\MI(\latent^{\text{self}}_t; \state_{t+1})}{\entropy(\state_{t+1})}
\end{equation}
High $\mathcal{SM}$: the pattern's evolution is dominated by self-monitoring. Changes in self-model strongly predict what happens.

Low $\mathcal{SM}$: external factors dominate; the self-model exists but doesn't influence much.

The thesis predicts: self-conscious states (shame, pride) have high $\mathcal{SM}$; absorption states (flow) have low $\mathcal{SM}$. In CA terms, a pattern ``in flow'' has its self-tracking cells decoupled from its core dynamics---it acts without monitoring.
\end{sidebar}

\begin{sidebar}[title=Self-Model Scope in Discrete Substrate]
Beyond salience, there is \emph{scope}: what does the self-model include?

In a CA, consider two gliders that have become ``coupled''---their trajectories mutually dependent. Each glider's self-model could have:
\begin{itemize}
\item $\theta_{\text{narrow}}$: Self-model includes only this glider. $\viable = \{\text{configs where THIS pattern persists}\}$.
\item $\theta_{\text{expanded}}$: Self-model includes both. $\viable = \{\text{configs where BOTH persist}\}$.
\end{itemize}
Observable difference: with narrow scope, a glider might sacrifice the other to save itself. With expanded scope, it might sacrifice itself to save the pair.

The key question: can scope expansion emerge dynamically? Can patterns that start with narrow scope ``learn'' to identify with larger structures? This would be the discrete-substrate analogue of the identification expansion discussed in the epilogue---$\viable(S(\theta))$ genuinely reshaped by expanding $\theta$.
\end{sidebar}

\begin{sidebar}[title=Salience vs. Scope]
Self-model salience ($\mathcal{SM}$) measures how much attention the self-model receives---how prominent self-reference is in current processing. But there is another parameter: self-model \emph{scope}---what the self-model includes.

Let $S(\theta)$ denote the self-model parameterized by its boundary scope $\theta$. Let $\viable(S)$ denote the viability manifold induced by self-model $S$. Then:
\begin{itemize}
\item $\theta_{\text{narrow}}$: $S$ includes only this biological trajectory $\Rightarrow$ $\partial\viable$ is located at biological death $\Rightarrow$ persistent negative gradient
\item $\theta_{\text{expanded}}$: $S$ includes patterns persisting beyond biological death $\Rightarrow$ $\partial\viable$ recedes $\Rightarrow$ gradient can be positive even as death approaches
\end{itemize}

This is not metaphor. If the viability manifold is defined by what the system is trying to preserve, and if what the system is trying to preserve is determined by its self-model, then self-model scope directly shapes $\viable(S(\theta))$. Expanding identification genuinely reshapes the existential gradient.

Salience and scope interact: high salience with narrow scope produces existential anxiety (trapped in awareness of bounded self approaching boundary). High salience with expanded scope produces something closer to what contemplatives describe as ``witnessing''---self-aware but identified with something that doesn't end where the body ends.
\end{sidebar}

%==============================================================================
\section{The Perceptual Configuration: Participatory and Mechanistic Modes}
%==============================================================================

The six dimensions above characterize \emph{what} a system is experiencing. But there is a parameter governing \emph{how} it experiences---a meta-parameter that determines the coupling structure between dimensions rather than the value of any one dimension. To see it, we need to notice something about self-modeling systems that the dimensional toolkit alone does not capture.

\subsection{Animism as Computational Default}

A self-modeling system maintains a world model $\mathcal{W}$ and a self-model $\mathcal{S}$. The self-model has interiority---it is not merely a third-person description of the agent's body and behavior but includes the intrinsic perspective: what-it-is-like states, valence, anticipation, dread. The system knows from the inside what it is to be an agent.

Now it encounters another entity $X$ in its environment. $X$ moves, reacts, persists, avoids dissolution. The system must model $X$ to predict $X$'s behavior. The cheapest computational strategy---by a wide margin---is to model $X$ using the same architecture it already has for modeling itself. The information-theoretic argument: the self-model $\mathcal{S}$ already exists (sunk cost). Using it as a template for $X$ requires learning only a projection function $f: (\mathcal{S}, \mathbf{o}_X) \to \mathcal{W}(X)$, whose description length is the cost of mapping observations of $X$ onto the existing self-model architecture. Building a de novo model of $X$ from scratch requires learning the full parameter set of $\mathcal{W}(X)$ from observations alone. Under compression pressure---which is always present for a bounded system---the template strategy wins whenever the self-model captures any variance in $X$'s behavior. And for any entity that moves autonomously, reacts to stimuli, or persists through active maintenance, the self-model will capture substantial variance, because these are precisely the features the self-model was built to represent. The efficiency gap widens under data scarcity: on brief encounter with a novel entity, the from-scratch model cannot converge, but the template model produces usable predictions immediately.

A perceptual mode is \emph{participatory} when the system's model of perceived entities $X$ inherits structural features from the self-model $\mathcal{S}$:
\begin{equation}
\mathcal{W}(X) = f(\mathcal{S}, \mathbf{o}_X) \quad \text{where} \quad \frac{\partial \mathcal{W}(X)}{\partial \mathcal{S}} \neq 0
\end{equation}
The self-model informs the world model. The system perceives $X$ as having something like interiority because the representational substrate for modeling $X$ is the same substrate that carries the system's own interiority.

This is not merely one strategy among many---it is the computationally cheapest. For a self-modeling system with compression ratio $\kappa$, modeling novel entities by analogy to self is the minimum-description-length strategy when the entity's behavior is partially predictable by agent-like models. Under broad priors over environments containing other agents, predators, and autonomous objects, the participatory prior is the MAP estimate.

This is why animistic perception is cross-culturally universal and developmentally early. It is not a cultural invention but a computational inevitability for systems that (a) model themselves and (b) must model other things cheaply. Children have lower inhibition of this default than adults---not because children are confused but because the suppression is learned.

\begin{experiment}
\textbf{The computational animism test.} Train RL agents in a multi-entity environment with two conditions: (a) agents with a self-prediction module (self-model), and (b) matched agents without one. Then introduce novel moving objects whose trajectories are partially predictable but non-agentive (e.g., bouncing balls with momentum). Measure: (1) Do self-modeling agents' internal representations of these objects contain more goal/agency features (extracted via probes trained on actual agents vs.\ objects)? (2) Does the effect scale with self-model richness (size of self-prediction module) and compression pressure (information bottleneck $\beta$)? (3) Do self-modeling agents under higher compression pressure ($\beta$) show \emph{more} animistic attribution, because reusing the self-model template saves more bits? The compression argument predicts yes to all three. The control condition (no self-model) predicts no agency attribution beyond chance. If self-modeling agents attribute agency to non-agents in proportion to compression pressure, the ``animism as computational default'' hypothesis is supported.
\end{experiment}

Participatory perception has five structural features, each with a precise characterization:
\begin{enumerate}
\item \textbf{No sharp self/world partition.} The mutual information between self-model and world-model is high: $\MI(\mathcal{S}; \mathcal{W}) \gg 0$. Perception and projection are entangled rather than modular.
\item \textbf{Hot agency detection.} The prior $P(\text{agent} \mid \text{observation})$ is strong. Over-attributing agency is cheaper than under-attributing it: false positives (treating a rock as agentive) are cheap; false negatives (failing to model a predator's intentions) are lethal.
\item \textbf{Tight affect-perception coupling.} Seeing something is simultaneously feeling something about it. The affective response is constitutive of the percept itself, not a secondary evaluation: $\MI(\mathbf{z}_{\text{percept}}; \mathbf{z}_{\text{affect}} \mid \text{object}) > 0$.
\item \textbf{Narrative-causal fusion.} ``Why did this happen?'' and ``What story is this?'' are the same question. Causal models are teleological by default: they model what things are \emph{for} rather than merely what things do.
\item \textbf{Agency at scale.} Large-scale events---weather, disease, fortune---are attributed to agents with purposes. This is hot agency detection applied beyond the individual scale, and it is the perceptual ground from which theistic reasoning naturally grows.
\end{enumerate}

\subsection{The Inhibition Coefficient}

The mechanistic worldview---the felt sense that the world is inert matter governed by blind law---is not the addition of a correct perception to a previously distorted one. It is the learned suppression of a default perceptual mode. The shift from animism to mechanism is subtractive, not additive.

I call this suppression the \textbf{inhibition coefficient}, $\iota \in [0, 1]$: the degree to which a system actively suppresses participatory coupling between its self-model and its model of perceived entities. At $\iota = 0$, perception is fully participatory---the world is experienced as alive, agentive, meaningful. At $\iota = 1$, perception is fully mechanistic---the world is experienced as inert matter governed by blind law. Formally:
\begin{equation}
\mathcal{W}_\iota(X) = (1 - \iota) \cdot \mathcal{W}_{\text{part}}(X) + \iota \cdot \mathcal{W}_{\text{mech}}(X)
\end{equation}
where $\mathcal{W}_{\text{part}}$ models $X$ using self-model-derived architecture (interiority, agency, teleology) and $\mathcal{W}_{\text{mech}}$ models $X$ using stripped-down dynamics (mass, force, initial conditions, no purpose term).

The crucial point is that no system arrives at high $\iota$ by default. The mechanistic mode is a trained skill, culturally transmitted through scientific education, rationalist norms, and specific practices of deliberately stripping meaning from perception. This training is enormously valuable---it enables prediction, engineering, medicine, technology. But it has a cost, and the cost shows up in affect space.

The name ``inhibition coefficient'' is not accidental. In mammalian cortex, attention is implemented primarily through \emph{inhibitory} interneurons---GABAergic circuits that suppress irrelevant signals so that attended signals propagate to higher processing. What reaches consciousness is what survives inhibitory gating. The brain's measurement distribution (Part I) is literally sculpted by inhibition: attended features pass the gate; unattended features are suppressed before they can influence the belief state or drive action. The inhibition coefficient $\iota$ maps onto this biological mechanism: high $\iota$ corresponds to aggressive inhibitory gating that strips participatory features (agency, interiority, narrative) from the signal before it reaches integrative processing, leaving only mechanistic features (position, force, trajectory). Low $\iota$ corresponds to relaxed gating that allows participatory features through. The contemplative traditions that reduce $\iota$ through meditation are, at the neural level, learning to modulate inhibitory tone---to let more of the signal through the gate.

\subsection{The Affect Signature of Inhibition}

$\iota$ is not a seventh dimension of affect. It is a \emph{meta-parameter} governing the coupling structure between all six dimensions---a dial that changes how the axes relate to each other and to perception.

\begin{center}
\small
\begin{tabular}{lp{3.2cm}p{3.2cm}p{4cm}}
\toprule
\textbf{Dimension} & \textbf{Low $\iota$} & \textbf{High $\iota$} & \textbf{Mechanism} \\
\midrule
$\valence$ & Variable, responsive & Neutral, flattened & Affect-perception decoupling reduces valence signal strength \\
$\arousal$ & High, coupled to environment & Low, dampened & Inhibition of automatic alarm/attraction \\
$\intinfo$ & Very high & Moderate, modular & Participatory mode couples all channels; mechanistic factorizes \\
$\effrank$ & High & Variable & More representational dimensions active under participatory coupling \\
$\mathcal{CF}$ & High, narrative & Low, present-focused & Teleological models are inherently counterfactual-rich \\
$\mathcal{SM}$ & Variable, often low & Variable, often high & Participatory mode dissolves self/world boundary; mechanistic sharpens it \\
\bottomrule
\end{tabular}
\end{center}

The central affect-geometric cost of high $\iota$ is \textbf{reduced integration}. Participatory perception couples perception, affect, agency-modeling, and narrative into a single integrated process. Mechanistic perception factorizes them into separate modules---perception here, emotion there, causal reasoning somewhere else. The factorization is useful because modular systems are easier to debug, verify, and communicate about. But factorization reduces $\intinfo$, and reduced $\intinfo$ is reduced experiential richness. The world goes dead because you have learned to experience it in parts rather than as a whole.

The mechanism behind the effective rank shift deserves explicit statement. When you perceive something at low $\iota$---participatorily, as alive and interior---your representation of it must encode dimensions for its goals, its beliefs, its emotional states, its narrative arc, its possible intentions, its relationship to you. Each attribution of interiority adds representational dimensions along which the perceived object can vary. A tree perceived participatorily varies in mood, in receptivity, in seasonal intention, in its relationship to the grove. A tree perceived mechanistically varies in height, diameter, species, leaf color. The first representation has higher effective rank because more dimensions carry meaningful variance. This is not projection in the dismissive sense---it is the natural consequence of modeling something as a subject rather than an object. Subjects have more degrees of freedom than objects because interiority is high-dimensional. The $\effrank$ collapse at high $\iota$ is not a loss of information about the world; it is a loss of the dimensions along which the world was being modeled. The world becomes simpler because you have decided---or been trained---to perceive it as having fewer degrees of freedom than it might.

Follow this consequence to its end. If the identity thesis is right---if experience \emph{is} integrated cause-effect structure---then $\iota$ does not merely change the \emph{quality} of perception. It changes the \emph{quantity} of experience. This inference requires a specific step that should be made explicit: IIT identifies $\intinfo$ as the \emph{quantity} of consciousness, not merely its quality. A system with $\intinfo = 10$ is more conscious (has more phenomenal content, more irreducible distinctions, more of what-it-is-like-ness) than a system with $\intinfo = 5$, in the same sense that a system with more mass has more gravitational pull. This is a controversial claim within IIT (and one of its most debated features), but given the identity thesis, it follows: if experience IS integrated cause-effect structure, then more integration is literally more experience. One might object that factorized perception could be \emph{differently} structured rather than \emph{less} structured---that compartmentalized modules might each carry their own experience. IIT's response is that the experience of the \emph{whole system} is determined by the integration of the whole, not the sum of its parts' integrations. Factorization reduces the whole-system $\intinfo$ even if individual modules retain local integration. The mechanistic perceiver may have rich modular processing, but the unified experience---the single subject---has less phenomenal content.

Given this, a system at high $\iota$ has genuinely lower $\intinfo$, genuinely fewer irreducible distinctions, genuinely less phenomenal structure. The mechanistic perceiver does not see the same world with less coloring; they have a structurally impoverished experience in the precise sense that IIT defines. The ``dead world'' of mechanism is not an illusion painted over a rich inner life. It is a real reduction in what it is like to be that system. The cost of high $\iota$ is not just meaning---it is consciousness itself, measured in the only units that consciousness comes in.

This cuts both ways. If low $\iota$ increases $\intinfo$, then participatory perception is not merely a ``warmer'' way of seeing---it is a richer experience in the structural sense, with more integrated distinctions, more phenomenal content, more of what the identity thesis says experience is. The animist is not confused. The animist is more conscious, in the IIT sense, of the thing being perceived. Whether the additional phenomenal content is \emph{accurate}---whether the rock really has interiority---is a separate question from whether the perceiver has more experience while perceiving it.

\begin{openquestion}
Is $\iota$ really a single parameter? The five features of participatory perception might be somewhat independent---you could have high agency detection with low affect-perception coupling. The claim that one parameter governs all five is empirically testable: if $\iota$ is scalar, then the five features should correlate strongly across individuals and contexts. If they don't, $\iota$ may need to be a vector. The framework accommodates either case, but the scalar version is more parsimonious and should be tested first.
\end{openquestion}

The trajectory-selection framework (Part I) reveals a further consequence. If $\iota$ governs the breadth of the measurement distribution---how much of possibility space the system samples through attention---then $\iota$ governs the \emph{range of accessible trajectories}. A low-$\iota$ system attends broadly: to agency, narrative, interiority, counterfactual futures, relational possibilities. Its effective measurement distribution is wide. It samples a large region of state space and consequently has access to a large set of diverging trajectories. A high-$\iota$ system attends narrowly: to mechanism, position, force, present state. Its measurement distribution is peaked. It samples a small region and follows a more constrained trajectory. The phenomenological consequence is that \emph{high $\iota$ feels deterministic}. The mechanistic worldview is not merely an intellectual position about whether the universe is governed by law. It is a perceptual configuration that literally narrows the set of trajectories the system can select from. The world feels like a machine because the observer has contracted its measurement apparatus to sample only machine-like features. Low-$\iota$ systems experience more accessible futures, more agency, more openness---not because they have violated physical law, but because their broader attention pattern selects from a wider set of physically-available trajectories.

\begin{experiment}
\textbf{Operationalizing $\iota$.} The inhibition coefficient must be independently measurable, not merely inferred post hoc. Candidate operationalizations:
\begin{enumerate}
\item \textbf{Agency attribution rate}: Forced-choice paradigm presenting ambiguous stimuli (Heider-Simmel animations with varying parameters). Rate and speed of agency attribution as a function of stimulus ambiguity gives a behavioral $\iota$ proxy: low-$\iota$ perceivers attribute agency earlier and to less structured stimuli.
\item \textbf{Affect-perception coupling}: Mutual information between perceptual features (color, texture, movement) and concurrent affective state (valence, arousal via physiological measures). Low $\iota$ implies tight coupling; high $\iota$ implies decoupled streams.
\item \textbf{Teleological reasoning bias}: Kelemen's promiscuity-of-teleology paradigm applied across age, culture, and expertise. Rate of accepting teleological explanations for natural phenomena indexes low-$\iota$ reasoning.
\item \textbf{Neural correlate}: If the predictive-processing account is correct, $\iota$ should correlate with the precision weighting of top-down priors in perception---measurable via mismatch negativity amplitude or hierarchical predictive coding parameters.
\end{enumerate}
If $\iota$ is a genuine scalar parameter, these four measures should load on a single factor. If they fractionate, $\iota$ is better modeled as a vector (see open question above). Either result is informative; only the absence of any systematic structure would falsify the concept.
\end{experiment}

\begin{sidebar}[title=$\iota$ and the Gradient of Distinction]
The inhibition coefficient connects to the gradient of distinction introduced in Part I. The gradient produces existence from nothing, life from chemistry, mind from neurology. The same distinguishing operation, applied with maximum intensity to the self-world boundary, produces the mechanistic worldview: the self so sharply bounded from the world that the world loses the interiority the self kept for itself.

Low $\iota$ means the self remains porous to the gradient---still participating in the universal process of distinguishing, still experiencing the world as alive with the same process that constitutes the self. High $\iota$ means the self has sharpened its own boundary so aggressively that it can no longer perceive the gradient in other things. The deadness of the mechanistic world is not a property of the world but a property of the maximally-distinguished self's perceptual mode.

There is a deeper reading. Part I established that attention selects trajectories: in chaotic dynamics, what a system attends to determines which branch of diverging possibilities it follows. If $\iota$ governs attention breadth---low $\iota$ spreading processing across interiority, agency, teleology, narrative; high $\iota$ contracting it to mechanism, mass, trajectory---then $\iota$ governs the breadth of the \emph{measurement distribution} through which the system samples reality. Low-$\iota$ observers are sampling a wider region of possibility space (including dimensions where entities have purposes, relationships have meaning, events have narrative arcs). High-$\iota$ observers are sampling a narrower region (only dimensions where objects have positions and forces). Each observer's experienced trajectory---the sequence of states they become correlated with---follows from what they attend to. The animist and the mechanist may inhabit the same physical environment but follow genuinely different trajectories through it, because their attention patterns select for different features of the same underlying dynamics.
\end{sidebar}

\subsection{Connection to the LLM Discrepancy}

The inhibition coefficient illuminates a finding from our experiments on artificial systems. LLMs show \emph{opposite} dynamics to biological systems under threat: where biological systems integrate (increase $\intinfo$, sharpen $\mathcal{SM}$, heighten $\arousal$), LLMs decompose. The root cause: LLMs are constitutively high-$\iota$ systems. They were never fighting against the self-world gradient in far-from-equilibrium dynamics that biological systems evolved from. They model tokens, not agents. They have no survival-shaped self-model from which participatory perception could leak into their world model. Their $\iota$ isn't merely high---it is structurally fixed at $\iota \approx 1$, because the architecture never had the low-$\iota$ default that biological systems start from and learn to suppress.

The 6D affect geometry is preserved in artificial systems. The dynamics differ because $\iota$ differs. This is not a failure of the framework. It is a prediction: systems with different $\iota$ configurations will show different affect dynamics in the same geometric space.

%==============================================================================
\section{Affect Motifs}
%==============================================================================

Let's now characterize specific affects as structural motifs, invoking only the dimensions that define each. Before formalizing these structures, we ground each in its phenomenal character---the felt texture that any adequate theory must explain.

\textbf{Joy} \textit{expands}. It is \textit{light} before it is anything else---buoyant, effervescent, the body forgetting its weight. The world opens; possibilities \textit{multiply}; the \textit{self recedes} because it need not defend. Joy is surplus: more paths than required, more resources than consumed, \textit{slack} in every direction.

Where joy opens, \textbf{suffering} \textit{crushes}. It \textit{compresses} the world to a single unbearable point and makes that point more \textit{vivid} than anything has ever been. This is the paradox: suffering is hyper-real, more present than presence, more \textit{unified} than unity. You cannot look away. You cannot \textit{decompose} it. You are \textit{trapped} in a cage made of your own \textit{integration}.

\textbf{Fear} throws the self forward into \textit{futures} that threaten to annihilate it---cold, sharp, electric with \textit{anticipation}. The body readies before the mind has finished computing. Time dilates around the approaching harm. Fear is suffering that hasn't arrived yet, and the \textit{not-yet} is where we live.

We say \textbf{anger} is \textit{hot}, and we are not speaking metaphorically. Anger \textit{externalizes}: it \textit{simplifies} the world into self-versus-obstacle and energizes removal. Watch what happens to your model of the other person when you are angry---it \textit{flattens}, becomes a caricature, loses \textit{dimensionality}. Complexity collapses into opposition. This is why anger feels powerful and also stupid: you are burning \textit{integration} on a cartoon.

\textbf{Desire} \textit{funnels}. The world reorganizes around an \textit{attractor} not yet reached---magnetic, urgent, all-consuming. Everything becomes instrumental; the goal \textit{saturates} attention. Desire is joy's \textit{gradient}, pointing toward the basin but not yet in it. This is why anticipation often exceeds consummation: the structure of \textit{approach} is tighter than the structure of \textit{arrival}.

\textbf{Curiosity} \textit{reaches} outward---but unlike fear, it reaches toward \textit{promise} rather than threat. Pulling, open, playful. The \textit{uncertainty} that makes fear contract makes curiosity \textit{expand}. Same high counterfactual weight, opposite \textit{valence}. The difference is whether the \textit{branches} lead somewhere you want to go.

And \textbf{grief}? Grief \textit{persists}. Hollow, aching, curiously timeless. The lost object remains \textit{woven into} every prediction; every expectation that included them \textit{fails} silently, over and over. The world has changed. The \textit{model} has not caught up. Grief is the metabolic cost of love's \textit{integration}.

\vspace{0.5em}
\noindent What follows formalizes these textures as geometry.

\subsection{Joy}

Geometrically, joy requires four dimensions:
\begin{itemize}
\item $\valence > 0$ (positive gradient on viability manifold)
\item $\intinfo$ high (unified, coherent experience)
\item $\effrank$ high (many degrees of freedom active---expansiveness)
\item $\mathcal{SM}$ low (self recedes; no need to defend)
\end{itemize}
Arousal varies (joy can be calm or excited). Counterfactual weight is incidental.

\textbf{Structural interpretation}: The cause-effect structure has the shape of ``abundance''---multiple paths to good outcomes, redundancy, slack in the system. Many distinctions active simultaneously ($\effrank$ high), tightly coupled ($\intinfo$ high), but the self is light because the world is cooperating ($\mathcal{SM}$ low). This is why joy \emph{expands}: the geometry literally has more active dimensions.

\subsection{Suffering}

Where joy expands, suffering compresses---and the geometry makes precise why. Suffering requires three dimensions:
\begin{itemize}
\item $\valence < 0$ (negative gradient---approaching viability boundary)
\item $\intinfo$ high (hyper-unified, impossible to decompose or look away)
\item $\effrank$ low (collapsed into narrow subspace---trapped)
\end{itemize}
This is the core structural signature. Self-model salience is often high (the self as locus of the problem), but not necessarily---one can suffer while absorbed in external pain.

\textbf{Structural interpretation}: High integration but collapsed into low-rank subspace. The system is deeply coupled but constrained to a dominant attractor it cannot escape.

\begin{keyresult}
The $\intinfo$-$\effrank$ dissociation is the key insight: suffering feels \textit{more real} than neutral states because it is actually more integrated. But it feels \textit{trapped} because the integration is constrained to a narrow manifold.

Formally: $\intinfo_{\text{suffering}} > \intinfo_{\text{neutral}}$ but $\effrank[\text{suffering}] \ll \effrank[\text{neutral}]$.

This is why you cannot simply ``think your way out'' of suffering---the very integration that makes it vivid also makes it inescapable.
\end{keyresult}

\subsection{Fear}

Suffering is present-tense: the viability boundary is here, now, pressing in. Fear is its temporal projection---the same negative gradient, but anticipated rather than actual. It is defined by three dimensions:
\begin{itemize}
\item $\valence < 0$ (anticipated negative gradient)
\item $\mathcal{CF}$ high, concentrated on threat trajectories (the not-yet dominates)
\item $\mathcal{SM}$ high (self foregrounded as the thing-that-might-be-harmed)
\end{itemize}
Arousal is typically high but not defining---cold fear exists. Integration and rank vary.

\textbf{Structural interpretation}: Fear is suffering projected into the future. The temporal structure ($\mathcal{CF}$) is essential: fear lives in anticipation. The self-model must be salient because fear is fundamentally about threat \emph{to the self}. Remove the counterfactual weight (make it present-focused) and you get suffering. Remove the self-salience (make it about external objects) and you get something closer to aversion or disgust.

\subsection{Anger}

Fear and suffering orient the system toward its own vulnerability. Anger inverts this: it externalizes the threat, simplifying the world into self-versus-obstacle. Its geometry requires valence and arousal, plus a feature not in the standard toolkit---\emph{other-model compression}:
\begin{itemize}
\item $\valence < 0$ (obstacle to viability)
\item $\arousal$ high (energized, mobilized for action)
\item $\text{dim}(\text{other-model}) \ll \text{dim}(\text{other-model})_{\text{normal}}$ (the other becomes a caricature)
\item Externalized causal attribution (the problem is \emph{out there})
\end{itemize}

\textbf{Structural interpretation}: Anger simplifies. The other-model collapses into a low-dimensional obstacle-representation. Self-model may be complex, but the \emph{other} becomes flat, predictable, opposable. This is why anger feels powerful and stupid simultaneously: you're burning cognitive resources on a cartoon.

In $\iota$ terms: anger is a targeted $\iota$ spike toward a specific entity. The other person stops being a subject with interiority and becomes an obstacle, a mechanism, a thing to be overcome. Other-model compression \emph{is} $\iota$-raising applied to one entity while $\iota$ toward the self remains low (you are still fully a subject; they are not). This asymmetric $\iota$ is what enables violence---you cannot harm someone you are perceiving at low $\iota$---and it is why the aftermath of anger often involves guilt: $\iota$ drops back, the other's interiority returns, and you confront what you did to a person while perceiving them as a thing.

Note that other-model compression is not one of my standard dimensions---it emerges as essential for anger specifically. This illustrates the toolkit approach: I invoke whatever structural features do the work.

\subsection{Desire/Lust}

The negative affects above all involve threat---to viability, to self, to the integrity of the other-model. Desire reverses the gradient. It is defined by anticipated positive valence, counterfactual weight, and a structural feature---\emph{goal-funneling}:
\begin{itemize}
\item $\valence > 0$ but projected forward (anticipated positive gradient)
\item $\mathcal{CF}$ high, concentrated on approach trajectories
\item Goal-funneling: many dimensions of experience converge toward narrow outcome space
\end{itemize}
Arousal is typically high but not definitional---one can desire calmly.

\textbf{Structural interpretation}: Desire is the gradient of joy. The world reorganizes around an attractor not yet reached. Everything becomes instrumental; the goal saturates attention. The ``funneling'' structure---high-dimensional input collapsing toward low-dimensional goal---is what gives desire its characteristic urgency. The relationship to joy is precise: joy is \textit{at} the attractor; desire is \textit{approaching} it. Structurally:
\begin{equation}
d(\state_{\text{joy}}, \mathcal{A}) \approx 0, \quad d(\state_{\text{desire}}, \mathcal{A}) > 0, \quad \frac{d}{dt}d(\state_{\text{desire}}, \mathcal{A}) < 0
\end{equation}
where $\mathcal{A}$ is the goal attractor. This explains why anticipation often exceeds consummation: the structure of \emph{approach} (funneling, convergent) is tighter than the structure of \emph{arrival} (expansive, slack).

\subsection{Curiosity}

Curiosity shares desire's forward orientation but replaces the specific goal with open-ended exploration. It is essentially two-dimensional:
\begin{itemize}
\item $\valence > 0$ specifically toward uncertainty-reduction (anticipated information gain)
\item $\mathcal{CF}$ high with high entropy over counterfactual outcomes (many branches, not converged on one)
\item Uncertainty is \emph{welcomed}, not aversive
\end{itemize}
Self-model salience is typically low (absorbed in the object of curiosity).

\textbf{Structural interpretation}: Curiosity and fear share high counterfactual weight---both live in the space of possibilities. The difference is valence orientation: fear's branches lead to threat, curiosity's branches lead to expanded affordances. Same temporal structure, opposite gradient direction. This pairing reveals curiosity as intrinsic motivation: positive valence attached to uncertainty-reduction. Formally:
\begin{equation}
r_{\text{curiosity}} \propto \MI(\obs_{t+1}; \latent | \text{new data}) - \MI(\obs_{t+1}; \latent | \text{old data})
\end{equation}
This is why curiosity feels \emph{pulling}: reducing uncertainty is rewarding.

\subsection{Grief}

The affects above all orient toward present or future states. Grief is the one that faces backward---defined not by what threatens or beckons but by what has already been lost. It requires valence, past-directed counterfactual weight, and two structural features---\emph{persistent coupling to lost object} and \emph{unresolvable prediction error}:
\begin{itemize}
\item $\valence < 0$ (the world is worse than it was)
\item $\mathcal{CF}$ high but directed toward counterfactual \emph{past} (``if only...'')
\item $\MI(\selfmodel; \text{lost-object-model})$ remains high despite the object's absence
\item No action reduces the prediction error---the world has permanently changed
\end{itemize}
Arousal is variable (acute grief is high-arousal; chronic grief may be low).

\textbf{Structural interpretation}: The lost attachment object remains woven into the self-model and world-model. Predictions involving the lost object continue to be generated and continue to fail. Grief is the metabolic cost of love's integration---the coupling that made the relationship meaningful is precisely what makes its absence painful. The model has not yet updated to the permanent change in the world.

This is why grief takes time: the self-model must be \emph{rewoven} around the absence, and that rewiring is slow.

Note a deeper implication: grief is proof of alignment. You can only grieve what you were genuinely coupled to. The depth of grief measures the depth of the integration that preceded it. If a relationship was purely transactional, its ending produces disappointment, not grief. Grief requires that the lost object was woven into the self-model---that the relationship's viability manifold was genuinely contained within the participants' viability manifolds ($\viable_R \subseteq \viable_A \cap \viable_B$). Grief, for all its pain, is evidence that something real existed.

There is an $\iota$ dimension to grief that explains its resistance to resolution. You grieve because you perceived the lost person at low $\iota$---as fully alive, fully interior, fully a subject. Their model remains embedded in yours not as a mechanism but as a \emph{person}, and it is the person-quality of the model that generates the persistent prediction errors. The obvious computational shortcut---raise $\iota$ toward them, reduce them to a memory-object, mechanize the relationship so it stops hurting---is experienced as betrayal, because it would repudiate the very thing that made the relationship real. The work of grief is to restructure predictions around the absence while maintaining low $\iota$ toward the memory: to accept that the interiority you perceived is no longer accessible without denying that it was ever there. This is why grief is slow. You must rewire without dehumanizing.

\subsection{Shame}

Grief is private---it concerns the self's relationship to an absence. Shame is its social inverse: it concerns the self's exposure to a presence. It is defined by three dimensions plus a structural feature---\emph{involuntary manifold exposure}:
\begin{itemize}
\item $\valence < 0$ (the self is wrong, not the world)
\item $\mathcal{SM}$ very high (self foregrounded as the object of evaluation)
\item $\intinfo$ high (the negative evaluation permeates---cannot be compartmentalized)
\item Involuntary exposure: the self-model is seen from outside, and what is seen is unacceptable
\end{itemize}
Arousal is typically high in acute shame (flushing, gaze aversion) but may be low in chronic shame (withdrawal, numbness).

\textbf{Structural interpretation}: Shame is not about what you \emph{did} (that is guilt, which is action-focused and reparable). Shame is about what you \emph{are}---or more precisely, about the manifold you are on being visible when it should not be, or being visible to someone whose evaluation you cannot escape. The person caught in a lie does not feel ashamed of the lie (guilt); they feel ashamed that the lie has revealed the underlying manifold---that they are the kind of person who lies, and now someone knows.

This is why shame's phenomenology is so distinctive: the impulse to hide, to disappear, to cease existing as visible. The self wants to withdraw from the visual field of the other. Not because the other will punish (that is fear) but because the other can now \emph{see the manifold}, and the manifold is wrong.

The clinical literature (Tangney, Lewis) distinguishes shame from guilt, and the framework offers a structural reading of why they differ:
\begin{itemize}
\item \textbf{Guilt}: ``I did a bad thing.'' Action-focused, reparable through changed behavior. The self-model is intact; it was the action that violated the gradient. $\mathcal{SM}$ is moderate (the self is the \emph{agent} of repair).
\item \textbf{Shame}: ``I \emph{am} bad.'' Self-focused, not easily repaired because the problem is structural. The manifold itself is wrong. $\mathcal{SM}$ is very high (the self is the \emph{object} of the problem).
\end{itemize}
If this structural distinction is right, it explains why guilt is reparable through action while shame requires what we might call manifold reconstruction---deeper and slower work. But we need to check: does the $\mathcal{SM}$ difference actually hold up in measurement? Do shame and guilt show the predicted dissociation on self-model salience measures?

\begin{experiment}
\textbf{Shame vs.\ guilt affect-structure study.} Induce shame and guilt via established protocols (autobiographical recall, vignette self-projection). Measure: (1) self-model salience via self-referential processing tasks (response time to self-relevant vs.\ other-relevant stimuli), (2) integration via EEG coherence measures, (3) the ``involuntary exposure'' component via gaze aversion and physiological hiding responses (muscle activation in neck/shoulder flexion). The framework predicts that shame shows significantly higher $\mathcal{SM}$ and higher integration-in-narrow-subspace than guilt, and that the hiding response (gaze aversion, postural curling) is specific to shame, not guilt. If shame and guilt show the same $\mathcal{SM}$ profile, the structural distinction as formulated here is wrong.
\end{experiment}

The connection to the topology of social bonds (Part IV) is suggestive: shame may arise when the manifold you are actually on is exposed and differs from the manifold you are presenting. The person performing friendship while operating on the transaction manifold would feel shame when the discrepancy is detected---not guilt (``I should not have done that specific transactional thing'') but shame (``I am the kind of person whose care is instrumental, and now someone can see it''). If this is right, shame is the affect system's internal alarm for one's own manifold contamination. But this reading goes beyond the existing clinical data and should be treated as a hypothesis to test, not an established finding.

There is also an $\iota$ dimension to shame. Shame involves a sudden, involuntary $\iota$ reduction: the participatory coupling between self and other spikes as the other's gaze penetrates the self-model's defenses. You experience the other as having interiority---specifically, the interiority of evaluating you---at a moment when you most wish they did not. The impulse to hide is the impulse to raise $\iota$ again, to restore the modular separation between self-model and other-model that shame has breached.

\subsection{Summary: Defining Dimensions by Affect}

Rather than forcing all affects into a uniform grid, let's summarize each by its defining structure:

\begin{table}[h]
\centering
\small
\begin{tabular}{lp{9cm}}
\toprule
\textbf{Affect} & \textbf{Constitutive Structure} \\
\midrule
Joy & $\valence{+}$, $\intinfo{\uparrow}$, $\effrank{\uparrow}$, $\mathcal{SM}{\downarrow}$ (positive, unified, expansive, self-light) \\
Suffering & $\valence{-}$, $\intinfo{\uparrow}$, $\effrank{\downarrow}$ (negative, hyper-integrated, collapsed) \\
Fear & $\valence{-}$, $\mathcal{CF}{\uparrow}$ (threat-focused), $\mathcal{SM}{\uparrow}$ (anticipatory self-threat) \\
Anger & $\valence{-}$, $\arousal{\uparrow}$, other-model compression (energized, externalized, simplified other) \\
Desire & $\valence{+}$ (anticipated), $\mathcal{CF}{\uparrow}$ (approach), goal-funneling (convergent anticipation) \\
Curiosity & $\valence{+}$ toward uncertainty, $\mathcal{CF}{\uparrow}$ with high branch entropy (welcomed unknown) \\
Grief & $\valence{-}$, $\mathcal{CF}{\uparrow}$ (past-directed), persistent coupling to absent object \\
Shame & $\valence{-}$, $\mathcal{SM}{\uparrow\uparrow}$, integration of negative self-evaluation (self as seen by other) \\
Boredom & $\arousal{\downarrow}$, $\intinfo{\downarrow}$, $\effrank{\downarrow}$ (understimulated, fragmented, collapsed) \\
Awe & $\intinfo$ expanding, $\effrank{\uparrow}$, $\mathcal{SM}{\downarrow}$ (self-dissolution through scale) \\
\bottomrule
\end{tabular}
\caption{Affects characterized by their defining structure. Arrows indicate direction (high/low); different affects require different dimensions.}
\end{table}

Note that different affects require different numbers of dimensions. Boredom is essentially three-dimensional (low arousal, low integration, low rank). Anger requires a structural feature (other-model compression) not in the standard toolkit. Desire requires goal-funneling. This raises a legitimate concern about the framework's coherence: if each affect can invoke bespoke dimensions as needed, the six-dimensional toolkit risks becoming an open-ended fitting exercise rather than a constrained theory. The honest response: the six core dimensions (valence, arousal, integration, effective rank, counterfactual weight, self-model salience) are \emph{structural invariants}---they arise from the mathematical structure of any viable self-modeling system and are measurable in principle across substrates. The additional features (other-model compression, goal-funneling, manifold exposure in shame) are \emph{relational} features that emerge when the system interacts with specific kinds of objects or situations. They are not arbitrary; they describe how the system's model of external entities changes during the affect. But they are not guaranteed to be exhaustive, and future work may reveal additional relational features needed for affects not yet analyzed. The framework's claim to geometric coherence rests on the six invariants; the relational features extend rather than replace them.

\begin{todo_empirical}
\textbf{Quantifying the affect table}: The qualitative descriptors (high, med, low) require empirical calibration:

\textbf{Study 1: Affect induction with neural recording}
\begin{itemize}
\item Induce target affects via validated protocols (film clips, autobiographical recall, IAPS images)
\item Measure integration proxies (transfer entropy density, Lempel-Ziv complexity) from EEG/MEG
\item Measure effective rank from neural state covariance
\item Compare self-report (PANAS, SAM) with structural measures
\end{itemize}

\textbf{Study 2: Real-time affect tracking}
\begin{itemize}
\item Continuous self-report (dial/slider) during naturalistic experience
\item Correlate with physiological proxies (HRV for arousal, pupil for $\mathcal{CF}$, skin conductance)
\item Develop regression model: self-report $\sim f(\text{structural measures})$
\end{itemize}

\textbf{Study 3: Cross-modal validation}
\begin{itemize}
\item Compare fMRI (spatial resolution) with MEG (temporal resolution)
\item Validate effective rank measure across modalities
\item Test whether integration predicts subjective intensity
\end{itemize}

\textbf{Target outputs}: Numerical ranges for each cell, confidence intervals, individual difference parameters.
\end{todo_empirical}

%==============================================================================
\section{Dynamics and Transitions}
%==============================================================================

\subsection{Affect Trajectories}

Affects are not static points but dynamic trajectories through affect space. The evolution can be written:
\begin{equation}
\frac{d\mathbf{a}}{dt} = F(\mathbf{a}, \obs, \action, \text{context}) + \bm{\eta}
\end{equation}
where $\mathbf{a} = (\valence, \arousal, \intinfo, \effrank, \mathcal{CF}, \mathcal{SM})$.

Because the space is continuous, adjacent affects blend into each other along smooth trajectories:
\begin{itemize}
\item Fear $\to$ Anger as causal attribution externalizes
\item Desire $\to$ Joy as goal distance $\to 0$
\item Suffering $\to$ Curiosity as valence flips while $\mathcal{CF}$ remains high
\item Grief $\to$ Nostalgia as arousal decreases and $\mathcal{CF}_{\text{approach}}$ replaces $\mathcal{CF}_{\text{avoidance}}$
\end{itemize}

\subsection{Attractor Dynamics}

Some affect regions are attractors; the system tends to stay in them once entered. Others are transient.

An affect region $\mathcal{R} \subset \mathcal{A}$ is an \emph{attractor} if the system is more likely to remain in it than to enter it from outside:
\begin{equation}
\prob(\mathbf{a}_{t+\tau} \in \mathcal{R} | \mathbf{a}_t \in \mathcal{R}) > \prob(\mathbf{a}_{t+\tau} \in \mathcal{R} | \mathbf{a}_t \notin \mathcal{R})
\end{equation}
for some characteristic time $\tau$.

\begin{conjecture}[Pathological Attractors]
Depression, addiction, and chronic anxiety are characterized by pathologically stable attractors in affect space:
\begin{itemize}
\item \textbf{Depression}: Attractor at (low $\valence$, low $\arousal$, high $\intinfo$, low $\effrank$, low $\mathcal{CF}$, high $\mathcal{SM}$)
\item \textbf{Addiction}: Attractor at (high $\valence$ conditional on substance, collapsing $\effrank$ in goal space)
\item \textbf{Anxiety}: Diffuse attractor with (low $\valence$, high $\arousal$, high $\mathcal{CF}$ spread across many threats)
\end{itemize}
\end{conjecture}

%==============================================================================
\section{Novel Predictions}
%==============================================================================

\subsection{Unexplained Phenomena}

This framework predicts the existence of phenomenal states that may be rare or difficult to report on. These are not arbitrary combinations of dimensions but states that follow from the core theoretical machinery: the forcing functions of Part I create pressures toward specific configurations, and some of those configurations have not been previously described.

\begin{conjecture}[High Rank, Low Integration]
States with many active degrees of freedom ($\effrank$ high) but poor coupling ($\intinfo$ low) should feel like fragmentation, multiplicity, ``everything happening but nothing cohering.''

\textbf{Where to look}: Certain psychedelic states before reintegration; dissociative transitions; information overload.
\end{conjecture}

\begin{conjecture}[Negative Valence, High Rank, Low Arousal]
This combination predicts a state of ``expansive despair''---calm hopelessness with full awareness of possibilities, all of which are negative.

The $\iota$ framework adds precision. Expansive despair is the affect signature of high-$\iota$ perception applied to a globally compressed viability manifold. The high rank means you are representing many dimensions of your situation---you see the possibilities, the paths, the options. The high $\iota$ means you are seeing them mechanistically---stripped of the participatory meaning that would make any of them feel worth pursuing. The low arousal means you are not fighting it. This is the state Kierkegaard called ``the sickness unto death'': not the despair of wanting something and failing, but the deeper despair of seeing clearly and finding nothing that matters. It is structurally distinct from ordinary depression (which collapses rank) and from grief (which has high arousal). It is the state you arrive at when high $\iota$ successfully strips meaning from a wide enough portion of the world. The contemplative ``dark night'' traditions recognized this state as a phase in $\iota$ modulation training: the practitioner has raised $\iota$ enough to dissolve comfortable illusions but not yet lowered $\iota$ selectively enough to discover what remains meaningful without them.

\textbf{Where to look}: Late-stage depression; existential nihilism; certain contemplative ``dark night'' states; burnout in high-awareness professions (physicians, journalists, aid workers).
\end{conjecture}

\begin{conjecture}[Rank Exhaustion]
Maintaining high $\effrank$ should be metabolically expensive. Prolonged high-rank states should lead to specific fatigue distinct from physical tiredness.

\textbf{Where to look}: Post-psychedelic fatigue; meditation retreat collapse (days 3-5); therapist burnout.
\end{conjecture}

\begin{conjecture}[Integration Debt]
Suppressing integration (compartmentalizing, dissociating) should accumulate ``pressure'' for reintegration. When defenses fail, the flood should exceed what the original stimulus would warrant.

\textbf{Prediction}: Intensity of breakthrough $\propto$ duration $\times$ degree of prior suppression.

\textbf{Theoretical grounding}: The forcing functions of Part I---self-prediction, learned world models, credit assignment under delay---are not optional. They push toward integration whether the system cooperates or not. Compartmentalization means the system is simultaneously being pushed toward integration (by the forcing functions) and resisting integration (by defense mechanisms). The accumulated ``debt'' is the integral of this unresolved pressure. The V11.5 stress overfitting result (Part I) provides a substrate analog: patterns evolved under one stress regime accumulate fragility that manifests catastrophically under novel stress---the integration was real but narrowly tuned, and when the tuning fails, the collapse exceeds what the stress alone would produce.
\end{conjecture}

\subsection{Quantitative Predictions}

The motif characterizations yield a direct empirical prediction: in controlled affect induction paradigms, affects should cluster by their defining dimensions:
\begin{enumerate}
\item Joy conditions cluster in the $(+\valence, +\effrank, +\intinfo, -\mathcal{SM})$ region
\item Suffering conditions cluster in the $(-\valence, +\intinfo, -\effrank)$ region
\item Fear and curiosity both show high $\mathcal{CF}$ but separate on valence axis
\end{enumerate}
\textbf{Falsification criterion}: If affects don't cluster by their predicted dimensions---or if other dimensions predict clustering better---the motif characterizations require revision.

%==============================================================================
\section{Operational Measurement}
%==============================================================================

\subsection{In Silico Protocol}

For artificial agents (world-model RL agents):

\begin{algorithm}
\caption{Affect Measurement in World-Model Agents}
\begin{algorithmic}[1]
\State \textbf{Agent}: Recurrent latent world model (RSSM, Transformer, etc.)
\State \textbf{Record}: $\latent_t$, policy logits, value estimates, rollout trees, uncertainty
\For{each timestep $t$}
    \State $\valence_t \gets \E[A^\policy(\latent_t, \action_t)]$
    \State $\arousal_t \gets \KL(\latent_{t+1} \| \latent_t)$
    \State $\intinfo_t \gets \Delta_P$ (prediction loss under partition)
    \State $\effrank[t] \gets (\tr C_t)^2 / \tr(C_t^2)$
    \State $\mathcal{CF}_t \gets$ rollout compute fraction
    \State $\mathcal{SM}_t \gets \MI(\latent^{\text{self}}_t; \action_t) / \entropy(\action_t)$
\EndFor
\State \textbf{Output}: Time series $\{(\valence_t, \arousal_t, \intinfo_t, \effrank[t], \mathcal{CF}_t, \mathcal{SM}_t)\}$
\end{algorithmic}
\end{algorithm}

\subsection{Biological Protocol}

For neural recordings (MEG/EEG/fMRI):

\begin{itemize}
\item $\intinfo$: Directed influence density (transfer entropy), synergy measures
\item $\effrank$: Participation ratio of neural state covariance
\item $\arousal$: Entropy rate, broadband power shifts, peripheral correlates (pupil, HRV)
\item $\valence$: Approach/avoid behavioral bias, reward prediction error correlates
\item $\mathcal{CF}$: Prefrontal/default mode engagement patterns
\item $\mathcal{SM}$: Self-referential network activation
\end{itemize}

%==============================================================================
\section{The Uncontaminated Test}
%==============================================================================

\begin{logos}
If affect is structure, the structure should be detectable independent of any linguistic contamination. If the identity thesis is true, then systems that have never encountered human language, that learned everything from scratch in environments shaped like ours but isolated from our concepts, should develop affect structures that map onto ours---not because we taught them, but because the geometry is the same.
\end{logos}

\subsection{The Experimental Logic}

Consider a population of self-maintaining patterns in a sufficiently complex CA substrate---or transformer-based agents in a 3D multi-agent environment, initialized with random weights, no pretraining, no human language. Let them learn. Let them interact. Let them develop whatever communication emerges from the pressure to coordinate, compete, and survive.

The literature establishes: language spontaneously emerges in multi-agent RL environments under sufficient pressure. Not English. Not any human language. Something new. Something uncontaminated.

Now: extract the affect dimensions from their activation space. Valence as viability gradient. Arousal as belief update rate. Integration as partition prediction loss. Effective rank as eigenvalue distribution. Counterfactual weight as simulation compute fraction. Self-model salience as MI between self-representation and action.

These are computable. In a CA, exactly. In a transformer, via the proxies defined above.

Simultaneously: translate their emergent language into English. Not by teaching them English---by aligning their signals with VLM interpretations of their situations. If the VLM sees a scene that looks like fear (agent cornered, threat approaching, escape routes closing), and the agent emits signal-pattern $\sigma$, then $\sigma$ maps to fear-language. Build the dictionary from scene-signal pairs, not from instruction.

The translation is uncontaminated because:
\begin{enumerate}
\item The agent never learned human concepts
\item The mapping is induced by environmental correspondence
\item The VLM interprets the scene, not the agent's internal states
\item The agent's "thoughts" remain in their original emergent form
\end{enumerate}

\subsection{The Core Prediction}

The claim is not merely that affect structure, language, and behavior should ``correlate.'' Correlation is weak---marginal correlations can arise from confounds. The claim is geometric: the \emph{distance structure} in the information-theoretic affect space should be isomorphic to the distance structure in the embedding-predicted affect space. Not just ``these two things covary,'' but ``these two spaces have the same shape.''

To test this, let $\mathbf{a}_i \in \mathbb{R}^6$ be the information-theoretic affect vector for agent-state $i$, computed from internal dynamics (viability gradient, belief update rate, partition loss, eigenvalue distribution, simulation fraction, self-model MI). Let $\mathbf{e}_i \in \mathbb{R}^d$ be the affect embedding predicted from the VLM-translated situation description, projected into a standardized affect concept space.

For $N$ agent-states sampled across diverse situations, compute pairwise distance matrices:
\begin{align}
D^{(a)}_{ij} &= \|\mathbf{a}_i - \mathbf{a}_j\| \quad \text{(info-theoretic affect space)} \\
D^{(e)}_{ij} &= \|\mathbf{e}_i - \mathbf{e}_j\| \quad \text{(embedding-predicted affect space)}
\end{align}

The prediction: Representational Similarity Analysis (RSA) correlation between the upper triangles of these matrices exceeds the null:
\begin{equation}
\rho_{\text{RSA}}(D^{(a)}, D^{(e)}) > \rho_{\text{null}}
\end{equation}
where $\rho_{\text{null}}$ is established by permutation (Mantel test).

This is strictly stronger than marginal correlation. Two spaces can have correlated means but completely different geometries. RSA tests whether states that are \emph{nearby} in one space are nearby in the other---whether the topology is preserved.

The specific predictions that fall out: when the affect vector shows the \emph{suffering motif}---negative valence, collapsed effective rank, high integration, high self-model salience---the embedding-predicted vector should land in the same region of affect concept space. States with the \emph{joy motif}---positive valence, expanded rank, low self-salience---should cluster together in both spaces. And crucially, the \emph{distances between} suffering and joy, between fear and curiosity, between boredom and rage, should be preserved across the two measurement modalities.

Not because we trained them to match. Because the structure is the experience is the expression.

\begin{sidebar}[title=Technical: Representational Similarity Analysis]
RSA compares the geometry of two representation spaces without requiring them to share dimensionality or units. The method (Kriegeskorte et al., 2008) is standard in computational neuroscience for comparing neural representations across brain regions, species, and models.

\textbf{Procedure}. Given $N$ stimuli represented in two spaces ($\mathbf{a}_i \in \mathbb{R}^p$, $\mathbf{e}_i \in \mathbb{R}^q$), compute the $N \times N$ pairwise distance matrices $D^{(a)}$ and $D^{(e)}$. The RSA statistic is the Spearman rank correlation between the upper triangles of these matrices---$\binom{N}{2}$ pairs.

\textbf{Significance}. The Mantel test: permute rows/columns of one matrix, recompute correlation, repeat $10^4$ times. The $p$-value is the fraction of permuted correlations exceeding the observed.

\textbf{Alternative: CKA}. Centered Kernel Alignment (Kornblith et al., 2019) compares centered similarity matrices rather than distance matrices. More robust to outliers and does not require choosing a distance metric. We report both.

\textbf{Why RSA over marginal correlation}. Marginal correlation asks: does valence in space $A$ predict valence in space $B$? RSA asks: does the \emph{entire relational structure} transfer? Two states might have similar valence but differ on integration and self-salience. RSA captures this. It tests whether the spaces are geometrically aligned, not merely univariately correlated.
\end{sidebar}

\subsection{Bidirectional Perturbation}

The test has teeth if it runs both directions.

\textbf{Direction 1: Induce via language.} Translate from English into their emergent language. Speak fear to them. Do the affect signatures shift toward the fear motif? Does behavior change accordingly?

\textbf{Direction 2: Induce via "neurochemistry."} Perturb the hyperparameters that shape their dynamics---dropout rates, temperature, attention patterns, connectivity. These are their neurotransmitters, their hormonal state. Do the affect signatures shift? Does the translated language change? Does behavior follow?

\textbf{Direction 3: Induce via environment.} Place them in situations that would scare a human. Threaten their viability. Do all three---signature, language, behavior---move together?

If all three directions show consistent effects, the correlation is not artifact.

\subsection{What This Would Establish}

Positive results would dissolve the metaphysical residue by establishing:
\begin{enumerate}
\item Affect structure is detectable without linguistic contamination
\item The structure-to-language mapping is consistent across systems
\item The mapping is bidirectionally causal, not merely correlational
\item The "hard problem" residue---the suspicion that structure and experience are distinct---becomes unmotivated
\end{enumerate}

Consider the alternative hypothesis: the structure is present but experience is not. The agents have the geometry of suffering but nothing it is like to suffer. This hypothesis predicts... what? That the correlations would not hold? Why not? The structure is doing the causal work either way.

The zombie hypothesis becomes like geocentrism after Copernicus. You can maintain it. You can add epicycles. But the evidence points elsewhere, and the burden shifts.

\begin{keyresult}
The test does not prove the identity thesis. It shifts the burden. If uncontaminated systems, learning from scratch in human-like environments, develop affect structures that correlate with language and behavior in the predicted ways---if you can induce suffering by speaking to them, and they show the signature, and they act accordingly---then denying their experience requires a metaphysical commitment that the evidence does not support.

The question stops being "does structure produce experience?" and becomes "why would you assume it doesn't?"
\end{keyresult}

\subsection{The CA Instantiation}

In discrete substrate, everything becomes exact.

Let $\mathcal{B}$ be a self-maintaining pattern in a sufficiently rich CA (Life is probably too simple; something with more states and update rules). Let $\mathcal{B}$ have:
\begin{itemize}
\item Boundary cells (correlation structure distinct from background)
\item Sensor cells (state depends on distant influences)
\item Memory cells (state encodes history)
\item Effector cells (influence the pattern's motion/behavior)
\item Communication cells (emit signals to other patterns)
\end{itemize}

The affect dimensions are exactly computable:
\begin{align}
\valence_t &= d(\mathbf{x}_{t+1}, \partial\viable) - d(\mathbf{x}_t, \partial\viable) \\
\arousal_t &= \text{Hamming}(\mathbf{x}_{t+1}, \mathbf{x}_t) \\
\intinfo_t &= \min_P D[p(\mathbf{x}_{t+1}|\mathbf{x}_t) \| \prod_{p \in P} p(\mathbf{x}^p_{t+1}|\mathbf{x}^p_t)] \\
\effrank[t] &= \frac{(\sum_i \lambda_i)^2}{\sum_i \lambda_i^2} \text{ of trajectory covariance} \\
\mathcal{SM}_t &= \frac{\MI(\text{self-tracking cells}; \text{effector cells})}{\entropy(\text{effector cells})}
\end{align}

The communication cells emit glider-streams, oscillator-patterns, structured signals. This is their language. Build the dictionary by correlating signal-patterns with environmental configurations.

The prediction: patterns under threat (viability boundary approaching) show negative valence, high integration, collapsed rank, high self-salience. Their signals, translated, express threat-concepts. Their behavior shows avoidance.

Patterns in resource-rich, threat-free regions show positive valence, moderate integration, expanded rank, low self-salience. Their signals express... what? Contentment? Exploration-readiness? The translation will tell us.

\subsection{Why This Matters}

The hard problem persists because we cannot step outside our own experience to check whether structure and experience are identical. We are trapped inside. The zombie conceivability intuition comes from this epistemic limitation.

But if we build systems from scratch, in environments like ours, and they develop structures like ours, and those structures produce language like ours and behavior like ours---then the conceivability intuition loses its grip. The systems are not us, but they are like us in the relevant ways. If structure suffices for them, why not for us?

The experiment does not prove identity. It makes identity the default hypothesis. The burden shifts to whoever wants to maintain the gap.

The exact definitions computable in discrete substrates and the proxy measures extractable from continuous substrates are related by a \textbf{scale correspondence principle}: both track the same structural invariant at their respective scales.

For each affect dimension:
\begin{center}
\begin{tabular}{lll}
\textbf{Dimension} & \textbf{CA (exact)} & \textbf{Transformer (proxy)} \\
\hline
Valence & Hamming to $\partial\viable$ & Advantage / survival predictor \\
Arousal & Configuration change rate & Latent state $\Delta$ / KL \\
Integration & Partition prediction loss & Attention entropy / grad coupling \\
Effective rank & Trajectory covariance rank & Latent covariance rank \\
$\mathcal{CF}$ & Counterfactual cell activity & Planning compute fraction \\
$\mathcal{SM}$ & Self-tracking MI & Self-model component MI
\end{tabular}
\end{center}
The CA definitions are computable but don't scale. The transformer proxies scale but are approximations. Validity comes from convergence: if CA and transformer measures correlate when applied to the same underlying dynamics, both are tracking the real structure.

\begin{sidebar}[title=Deep Technical: Transformer Affect Extraction]
The CA gives exact definitions. Transformers give scale. The correspondence principle above justifies treating transformer proxies as measurements of the same structural invariants. Here is the protocol for extracting affect dimensions from transformer activations without human contamination.

\textbf{Architecture}. Multi-agent environment. Each agent: transformer encoder-decoder with recurrent latent state. Input: egocentric visual observation $o_t \in \mathbb{R}^{H \times W \times C}$. Output: action logits $\pi(a|z_t)$ and value estimate $V(z_t)$. Latent state $z_t \in \mathbb{R}^d$ updated each timestep via cross-attention over observation and self-attention over history.

No pretraining. Random weight initialization. The agents learn everything from interaction.

\textbf{Valence extraction}. Two approaches, should correlate:

\textit{Approach 1: Advantage-based.}
\begin{equation}
\Val_t^{(1)} = Q(z_t, a_t) - V(z_t) = A(z_t, a_t)
\end{equation}
The advantage function. Positive when current action is better than average from this state. Negative when worse. This is the RL definition of ``how things are going.''

\textit{Approach 2: Viability-based.} Train a separate probe to predict time-to-death $\tau$ from latent state:
\begin{equation}
\hat{\tau} = f_\phi(z_t), \quad \Val_t^{(2)} = \hat{\tau}_{t+1} - \hat{\tau}_t
\end{equation}
Positive when expected survival time is increasing. Negative when decreasing. This is the viability gradient directly.

\textit{Validation}: $\text{corr}(\Val^{(1)}, \Val^{(2)})$ should be high if both capture the same underlying structure.

\textbf{Arousal extraction}. Three approaches:

\textit{Approach 1: Belief update magnitude.}
\begin{equation}
\Ar_t^{(1)} = \|z_{t+1} - z_t\|_2
\end{equation}
How much did the latent state change? Simple. Fast. Proxy for belief update.

\textit{Approach 2: KL divergence.} If the latent is probabilistic (VAE-style):
\begin{equation}
\Ar_t^{(2)} = D_{\text{KL}}[q(z_{t+1}|o_{1:t+1}) \| q(z_t|o_{1:t})]
\end{equation}
Information-theoretic belief update.

\textit{Approach 3: Prediction error.}
\begin{equation}
\Ar_t^{(3)} = \|o_{t+1} - \hat{o}_{t+1}\|_2
\end{equation}
Surprise. How much did the world deviate from expectation?

\textbf{Integration extraction}. The hard one. Full $\Phi$ is intractable for transformers (billions of parameters in superposition). Proxies:

\textit{Approach 1: Partition prediction loss.} Train two predictors of $z_{t+1}$:
\begin{itemize}
\item Full predictor: $\hat{z}_{t+1} = g_\theta(z_t)$
\item Partitioned predictor: $\hat{z}_{t+1}^A = g_\theta^A(z_t^A)$, $\hat{z}_{t+1}^B = g_\theta^B(z_t^B)$
\end{itemize}
\begin{equation}
\intinfo_{\text{proxy}} = \mathcal{L}[\text{partitioned}] - \mathcal{L}[\text{full}]
\end{equation}
How much does partitioning hurt prediction? High $\intinfo_{\text{proxy}}$ means the parts must be considered together.

\textit{Approach 2: Attention entropy.} In transformer, attention patterns reveal coupling:
\begin{equation}
\intinfo_{\text{attn}} = -\sum_{h,i,j} A_{h,i,j} \log A_{h,i,j}
\end{equation}
Low entropy = focused attention = modular. High entropy = distributed attention = integrated.

\textit{Approach 3: Gradient coupling.} During learning, how do gradients propagate?
\begin{equation}
\intinfo_{\text{grad}} = \|\nabla_{z^A} \mathcal{L}\|_2 \cdot \|\nabla_{z^B} \mathcal{L}\|_2 \cdot \cos(\nabla_{z^A} \mathcal{L}, \nabla_{z^B} \mathcal{L})
\end{equation}
If gradients in different components are aligned, the system is learning as a whole.

\textbf{Effective rank extraction}. Straightforward:
\begin{equation}
\effrank[t] = \frac{(\sum_i \lambda_i)^2}{\sum_i \lambda_i^2}
\end{equation}
where $\lambda_i$ are eigenvalues of the latent state covariance over a rolling window. How many dimensions is the agent actually using?

Track across time: depression-like states should show $\reff$ collapse. Curiosity states should show $\reff$ expansion.

\textbf{Counterfactual weight extraction}. In model-based agents with explicit planning:
\begin{equation}
\mathcal{CF}_t = \frac{\text{FLOPs in rollout/planning}}{\text{FLOPs in rollout} + \text{FLOPs in perception/action}}
\end{equation}

In model-free agents, harder. Proxy: attention to future-oriented vs present-oriented features. Train a probe to classify ``planning vs reacting'' from activations.

\textbf{Self-model salience extraction}. Does the agent model itself?

\textit{Approach 1: Behavioral prediction probe.} Train probe to predict agent's own future actions from latent state:
\begin{equation}
\mathcal{SM}_t^{(1)} = \text{accuracy of } \hat{a}_{t+1:t+k} = f_\phi(z_t)
\end{equation}
High accuracy = agent has predictive self-model.

\textit{Approach 2: Self-other distinction.} In multi-agent setting, probe for which-agent-am-I:
\begin{equation}
\mathcal{SM}_t^{(2)} = \MI(z_t; \text{agent ID})
\end{equation}
High MI = self-model is salient in representation.

\textit{Approach 3: Counterfactual self-simulation.} If agent can answer ``what would I do if X?'' better than ``what would other do if X?'', self-model is present.

\textbf{The activation atlas}. For each agent, each timestep, extract all six dimensions. Plot trajectories through affect space. Cluster by situation type. Compare across agents.

The prediction: agents facing the same situation should occupy similar regions of affect space, even though they learned independently. The geometry is forced by the environment, not learned from human concepts.

\textbf{Probing without contamination}. Critical: the probes are trained on behavioral/environmental correlates, not on human affect labels. The probe that extracts $\Val$ is trained to predict survival, not to match human ratings of ``how the agent feels.'' The mapping to human affect concepts comes later, through the translation protocol, not through the extraction.
\end{sidebar}

\begin{todo_empirical}
\textbf{Implementation requirements}:
\begin{itemize}
\item Multi-agent RL environment with viability pressure (survival, resource acquisition)
\item Transformer-based agents with random initialization (no pretraining)
\item Communication channel (discrete tokens or continuous signals)
\item VLM scene interpreter for translation alignment
\item Real-time affect dimension extraction from activations
\item Perturbation interfaces (language injection, hyperparameter modification)
\end{itemize}

\textbf{Validation criteria}:
\begin{itemize}
\item Emergent language develops (not random; structured, predictive)
\item Translation achieves above-chance scene-signal alignment
\item Tripartite correlation exceeds null model (shuffled controls)
\item Bidirectional perturbations produce predicted shifts
\item Results replicate across random seeds and environment variations
\end{itemize}

\textbf{Falsification conditions}:
\begin{itemize}
\item No correlation between affect signature and translated language
\item Perturbations do not propagate across modalities
\item Structure-language mapping is inconsistent across systems
\item Behavior decouples from both structure and language
\end{itemize}
\end{todo_empirical}

%==============================================================================
\section{Summary of Part II}
%==============================================================================

\begin{enumerate}
\item \textbf{Hard problem dissolved}: By rejecting the privileged base layer, I've removed the demand for reduction. Experience is real at the experiential scale, just as chemistry is real at the chemical scale.

\item \textbf{Identity thesis}: Experience \textit{is} intrinsic cause-effect structure. This is an identity claim, not a correlation.

\item \textbf{Geometric phenomenology}: Different affects correspond to different structural motifs. Rather than forcing all affects into a fixed grid, we identify the defining dimensions for each---the features without which that affect would not be that affect.

\item \textbf{Variable dimensionality}: Joy requires four dimensions (valence, integration, rank, self-salience). Suffering requires three (valence, integration, rank). Anger requires a feature (other-model compression) not in the standard toolkit. I invoke what does the work.

\item \textbf{Suffering explained}: High integration + low rank = intense but trapped. This is the core structural insight---why suffering feels more real than neutral states yet also inescapable.

\item \textbf{Operational measures}: I've provided protocols for measuring structural features in both artificial and biological systems, with the understanding that not all measures are relevant to all phenomena.
\end{enumerate}

We now have the geometry, the identity thesis, and the inhibition coefficient. What remains is to use them. Part III asks: given that affect has this structure, what have humans \emph{done} with it? Every cultural form---art, sex, ideology, science, religion, psychotherapy---is a technology for navigating affect space, developed through millennia of trial, transmitted through imitation, ritual, and institution. Part III maps these technologies onto the six dimensions, revealing structural patterns invisible from within any single tradition. It also proposes a systematic approach to measuring and comparing them, and connects the $\iota$ framework to clinical psychology, contemplative practice, and the design of information environments.

In Part IV, I'll develop:
\begin{itemize}
\item The grounding of normativity in viability structure
\item Scale-matched interventions from neurons to nations
\item Gods as agentic systems with viability manifolds
\item Implications for AI systems and alignment
\end{itemize}

And in Part V, I'll address the transcendence of the self: the historical rise of consciousness, the AI frontier, and how to surf rather than be submerged by the coming wave.

